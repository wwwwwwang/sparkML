2016-11-09 14:26:30,648  INFO  org.apache.spark.SparkContext - logInfo: Running Spark version 1.6.0
2016-11-09 14:26:31,109  INFO  org.apache.spark.SecurityManager - logInfo: Changing view acls to: root
2016-11-09 14:26:31,111  INFO  org.apache.spark.SecurityManager - logInfo: Changing modify acls to: root
2016-11-09 14:26:31,112  INFO  org.apache.spark.SecurityManager - logInfo: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
2016-11-09 14:26:31,274  INFO  org.apache.spark.util.Utils - logInfo: Successfully started service 'sparkDriver' on port 49328.
2016-11-09 14:26:31,543  INFO  akka.event.slf4j.Slf4jLogger - applyOrElse: Slf4jLogger started
2016-11-09 14:26:31,575  INFO  Remoting - apply$mcV$sp: Starting remoting
2016-11-09 14:26:31,670  INFO  Remoting - apply$mcV$sp: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.100.103:60307]
2016-11-09 14:26:31,671  INFO  Remoting - apply$mcV$sp: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@192.168.100.103:60307]
2016-11-09 14:26:31,676  INFO  org.apache.spark.util.Utils - logInfo: Successfully started service 'sparkDriverActorSystem' on port 60307.
2016-11-09 14:26:31,688  INFO  org.apache.spark.SparkEnv - logInfo: Registering MapOutputTracker
2016-11-09 14:26:31,699  INFO  org.apache.spark.SparkEnv - logInfo: Registering BlockManagerMaster
2016-11-09 14:26:31,708  INFO  org.apache.spark.storage.DiskBlockManager - logInfo: Created local directory at /tmp/blockmgr-46272d6b-df6e-4cf8-a7b0-4cf03e32383e
2016-11-09 14:26:31,719  INFO  org.apache.spark.storage.MemoryStore - logInfo: MemoryStore started with capacity 530.0 MB
2016-11-09 14:26:31,766  INFO  org.apache.spark.SparkEnv - logInfo: Registering OutputCommitCoordinator
2016-11-09 14:26:31,879  INFO  org.spark-project.jetty.server.Server - doStart: jetty-8.y.z-SNAPSHOT
2016-11-09 14:26:31,914  INFO  org.spark-project.jetty.server.AbstractConnector - doStart: Started SelectChannelConnector@0.0.0.0:4040
2016-11-09 14:26:31,915  INFO  org.apache.spark.util.Utils - logInfo: Successfully started service 'SparkUI' on port 4040.
2016-11-09 14:26:31,916  INFO  org.apache.spark.ui.SparkUI - logInfo: Started SparkUI at http://192.168.100.103:4040
2016-11-09 14:26:31,936  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/lib/mysql-connector-java-5.1.25.jar at spark://192.168.100.103:49328/jars/mysql-connector-java-5.1.25.jar with timestamp 1478672791935
2016-11-09 14:26:31,936  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/lib/ojdbc6.jar at spark://192.168.100.103:49328/jars/ojdbc6.jar with timestamp 1478672791936
2016-11-09 14:26:31,936  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/lib/orai18n.jar at spark://192.168.100.103:49328/jars/orai18n.jar with timestamp 1478672791936
2016-11-09 14:26:31,936  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/lib/sequoiadb-1.jar at spark://192.168.100.103:49328/jars/sequoiadb-1.jar with timestamp 1478672791936
2016-11-09 14:26:31,936  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/lib/spark-sequoiadb-2.10-1.12.jar at spark://192.168.100.103:49328/jars/spark-sequoiadb-2.10-1.12.jar with timestamp 1478672791936
2016-11-09 14:26:31,937  INFO  org.apache.spark.SparkContext - logInfo: Added JAR file:/opt/whsh/k-means/mlib.jar at spark://192.168.100.103:49328/jars/mlib.jar with timestamp 1478672791937
2016-11-09 14:26:32,048  INFO  org.apache.spark.util.Utils - logInfo: Copying /opt/whsh/k-means/config/core-site.xml to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/core-site.xml
2016-11-09 14:26:32,062  INFO  org.apache.spark.SparkContext - logInfo: Added file file:/opt/whsh/k-means/config/core-site.xml at file:/opt/whsh/k-means/config/core-site.xml with timestamp 1478672792047
2016-11-09 14:26:32,062  INFO  org.apache.spark.util.Utils - logInfo: Copying /opt/whsh/k-means/config/hdfs-site.xml to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/hdfs-site.xml
2016-11-09 14:26:32,064  INFO  org.apache.spark.SparkContext - logInfo: Added file file:/opt/whsh/k-means/config/hdfs-site.xml at file:/opt/whsh/k-means/config/hdfs-site.xml with timestamp 1478672792062
2016-11-09 14:26:32,065  INFO  org.apache.spark.util.Utils - logInfo: Copying /opt/whsh/k-means/config/hive-site.xml to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/hive-site.xml
2016-11-09 14:26:32,067  INFO  org.apache.spark.SparkContext - logInfo: Added file file:/opt/whsh/k-means/config/hive-site.xml at file:/opt/whsh/k-means/config/hive-site.xml with timestamp 1478672792065
2016-11-09 14:26:32,068  INFO  org.apache.spark.util.Utils - logInfo: Copying /opt/whsh/k-means/config/log4j.properties to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/log4j.properties
2016-11-09 14:26:32,070  INFO  org.apache.spark.SparkContext - logInfo: Added file file:/opt/whsh/k-means/config/log4j.properties at file:/opt/whsh/k-means/config/log4j.properties with timestamp 1478672792068
2016-11-09 14:26:32,115  INFO  org.apache.spark.executor.Executor - logInfo: Starting executor ID driver on host localhost
2016-11-09 14:26:32,128  INFO  org.apache.spark.util.Utils - logInfo: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30044.
2016-11-09 14:26:32,129  INFO  org.apache.spark.network.netty.NettyBlockTransferService - logInfo: Server created on 30044
2016-11-09 14:26:32,131  INFO  org.apache.spark.storage.BlockManager - logInfo: external shuffle service port = 7337
2016-11-09 14:26:32,131  INFO  org.apache.spark.storage.BlockManagerMaster - logInfo: Trying to register BlockManager
2016-11-09 14:26:32,133  INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - logInfo: Registering block manager localhost:30044 with 530.0 MB RAM, BlockManagerId(driver, localhost, 30044)
2016-11-09 14:26:32,134  INFO  org.apache.spark.storage.BlockManagerMaster - logInfo: Registered BlockManager
2016-11-09 14:26:32,832  INFO  org.apache.spark.scheduler.EventLoggingListener - logInfo: Logging events to hdfs://nameservice1/user/spark/applicationHistory/local-1478672792093
2016-11-09 14:26:33,456  INFO  org.apache.spark.sql.hive.HiveContext - logInfo: Initializing execution hive, version 1.1.0
2016-11-09 14:26:33,498  INFO  org.apache.spark.sql.hive.client.ClientWrapper - logInfo: Inspected Hadoop version: 2.6.0-cdh5.7.2
2016-11-09 14:26:33,498  INFO  org.apache.spark.sql.hive.client.ClientWrapper - logInfo: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.7.2
2016-11-09 14:26:33,719  INFO  hive.metastore - open: Trying to connect to metastore with URI thrift://hadoop101.dategeek.com.cn:9083
2016-11-09 14:26:33,736  INFO  hive.metastore - open: Opened a connection to metastore, current connections: 1
2016-11-09 14:26:33,756  INFO  hive.metastore - open: Connected to metastore.
2016-11-09 14:26:33,827  INFO  hive.ql.metadata.Hive - reloadFunctions: Registering function changetime ingeekdata.hiveudf.HiveUDF
2016-11-09 14:26:34,161  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created HDFS directory: file:/tmp/spark-bc196639-ea6f-4edf-b717-71fcc5e9aa93/scratch/root
2016-11-09 14:26:34,162  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created local directory: /tmp/2b575c31-8f4e-4be9-a426-04acb589e1c1_resources
2016-11-09 14:26:34,162  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created HDFS directory: file:/tmp/spark-bc196639-ea6f-4edf-b717-71fcc5e9aa93/scratch/root/2b575c31-8f4e-4be9-a426-04acb589e1c1
2016-11-09 14:26:34,163  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created local directory: /tmp/root/2b575c31-8f4e-4be9-a426-04acb589e1c1
2016-11-09 14:26:34,163  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created HDFS directory: file:/tmp/spark-bc196639-ea6f-4edf-b717-71fcc5e9aa93/scratch/root/2b575c31-8f4e-4be9-a426-04acb589e1c1/_tmp_space.db
2016-11-09 14:26:34,168  INFO  org.apache.hadoop.hive.ql.session.SessionState - start: No Tez session required at this point. hive.execution.engine=mr.
sql = select holding_time from jiangyin.entrance_log_time where holding_time < 4000 and no = 10 and holding_time > 0 
2016-11-09 14:26:34,855  INFO  org.apache.spark.sql.hive.HiveContext - logInfo: default warehouse location is /user/hive/warehouse
2016-11-09 14:26:34,861  INFO  org.apache.spark.sql.hive.HiveContext - logInfo: Initializing metastore client version 1.1.0 using Spark classes.
2016-11-09 14:26:34,874  INFO  org.apache.spark.sql.hive.client.ClientWrapper - logInfo: Inspected Hadoop version: 2.6.0-cdh5.7.2
2016-11-09 14:26:34,902  INFO  org.apache.spark.sql.hive.client.ClientWrapper - logInfo: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.7.2
2016-11-09 14:26:35,305  INFO  hive.metastore - open: Trying to connect to metastore with URI thrift://hadoop101.dategeek.com.cn:9083
2016-11-09 14:26:35,321  INFO  hive.metastore - open: Opened a connection to metastore, current connections: 1
2016-11-09 14:26:35,331  INFO  hive.metastore - open: Connected to metastore.
2016-11-09 14:26:35,395  INFO  hive.ql.metadata.Hive - reloadFunctions: Registering function changetime ingeekdata.hiveudf.HiveUDF
2016-11-09 14:26:35,670  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created local directory: /tmp/43d2742d-8cba-444d-b02f-b9e5d0478a7f_resources
2016-11-09 14:26:35,708  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created HDFS directory: /tmp/hive/root/43d2742d-8cba-444d-b02f-b9e5d0478a7f
2016-11-09 14:26:35,709  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created local directory: /tmp/root/43d2742d-8cba-444d-b02f-b9e5d0478a7f
2016-11-09 14:26:35,731  INFO  org.apache.hadoop.hive.ql.session.SessionState - createPath: Created HDFS directory: /tmp/hive/root/43d2742d-8cba-444d-b02f-b9e5d0478a7f/_tmp_space.db
2016-11-09 14:26:35,736  INFO  org.apache.hadoop.hive.ql.session.SessionState - start: No Tez session required at this point. hive.execution.engine=mr.
2016-11-09 14:26:36,217  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_0 stored as values in memory (estimated size 631.7 KB, free 631.7 KB)
2016-11-09 14:26:36,363  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.0 KB, free 677.7 KB)
2016-11-09 14:26:36,365  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_0_piece0 in memory on localhost:30044 (size: 46.0 KB, free: 530.0 MB)
2016-11-09 14:26:36,370  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 0 from javaRDD at Kmeans_new.java:37
2016-11-09 14:26:36,549  WARN  org.apache.spark.mllib.clustering.KMeans - logWarning: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.
2016-11-09 14:26:36,592  INFO  org.apache.hadoop.hive.ql.log.PerfLogger - PerfLogBegin: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
2016-11-09 14:26:36,594  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
2016-11-09 14:26:36,615  INFO  org.apache.hadoop.hive.ql.io.orc.OrcInputFormat - generateSplitsInfo: FooterCacheHitRatio: 0/1
2016-11-09 14:26:36,615  INFO  org.apache.hadoop.hive.ql.log.PerfLogger - PerfLogEnd: </PERFLOG method=OrcGetSplits start=1478672796592 end=1478672796615 duration=23 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
2016-11-09 14:26:36,622  INFO  org.apache.spark.SparkContext - logInfo: Starting job: takeSample at KMeans.scala:378
2016-11-09 14:26:36,634  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 0 (takeSample at KMeans.scala:378) with 1 output partitions
2016-11-09 14:26:36,634  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 0 (takeSample at KMeans.scala:378)
2016-11-09 14:26:36,634  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:36,638  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:36,644  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 0 (MapPartitionsRDD[9] at map at KMeans.scala:210), which has no missing parents
2016-11-09 14:26:36,707  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_1 stored as values in memory (estimated size 13.5 KB, free 691.2 KB)
2016-11-09 14:26:36,717  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KB, free 698.0 KB)
2016-11-09 14:26:36,718  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_1_piece0 in memory on localhost:30044 (size: 6.8 KB, free: 530.0 MB)
2016-11-09 14:26:36,719  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:36,721  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at map at KMeans.scala:210)
2016-11-09 14:26:36,723  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 0.0 with 1 tasks
2016-11-09 14:26:36,755  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2962 bytes)
2016-11-09 14:26:36,761  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 0.0 (TID 0)
2016-11-09 14:26:36,765  INFO  org.apache.spark.executor.Executor - logInfo: Fetching file:/opt/whsh/k-means/config/hdfs-site.xml with timestamp 1478672792062
2016-11-09 14:26:36,767  INFO  org.apache.spark.ExecutorAllocationManager - logInfo: New executor driver has registered (new total is 1)
2016-11-09 14:26:36,783  INFO  org.apache.spark.util.Utils - logInfo: /opt/whsh/k-means/config/hdfs-site.xml has been previously copied to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/hdfs-site.xml
2016-11-09 14:26:36,786  INFO  org.apache.spark.executor.Executor - logInfo: Fetching file:/opt/whsh/k-means/config/hive-site.xml with timestamp 1478672792065
2016-11-09 14:26:36,786  INFO  org.apache.spark.util.Utils - logInfo: /opt/whsh/k-means/config/hive-site.xml has been previously copied to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/hive-site.xml
2016-11-09 14:26:36,789  INFO  org.apache.spark.executor.Executor - logInfo: Fetching file:/opt/whsh/k-means/config/log4j.properties with timestamp 1478672792068
2016-11-09 14:26:36,790  INFO  org.apache.spark.util.Utils - logInfo: /opt/whsh/k-means/config/log4j.properties has been previously copied to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/log4j.properties
2016-11-09 14:26:36,792  INFO  org.apache.spark.executor.Executor - logInfo: Fetching file:/opt/whsh/k-means/config/core-site.xml with timestamp 1478672792047
2016-11-09 14:26:36,793  INFO  org.apache.spark.util.Utils - logInfo: /opt/whsh/k-means/config/core-site.xml has been previously copied to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/core-site.xml
2016-11-09 14:26:36,798  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/ojdbc6.jar with timestamp 1478672791936
2016-11-09 14:26:36,829  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/ojdbc6.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp5801358039869998835.tmp
2016-11-09 14:26:36,880  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/ojdbc6.jar to class loader
2016-11-09 14:26:36,881  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/sequoiadb-1.jar with timestamp 1478672791936
2016-11-09 14:26:36,881  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/sequoiadb-1.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp4822953975330199178.tmp
2016-11-09 14:26:36,888  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/sequoiadb-1.jar to class loader
2016-11-09 14:26:36,888  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/mlib.jar with timestamp 1478672791937
2016-11-09 14:26:36,889  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/mlib.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp9029464474997810709.tmp
2016-11-09 14:26:37,130  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/mlib.jar to class loader
2016-11-09 14:26:37,131  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/orai18n.jar with timestamp 1478672791936
2016-11-09 14:26:37,132  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/orai18n.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp5937217581960336995.tmp
2016-11-09 14:26:37,139  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/orai18n.jar to class loader
2016-11-09 14:26:37,139  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/spark-sequoiadb-2.10-1.12.jar with timestamp 1478672791936
2016-11-09 14:26:37,140  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/spark-sequoiadb-2.10-1.12.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp5913909941596720276.tmp
2016-11-09 14:26:37,145  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/spark-sequoiadb-2.10-1.12.jar to class loader
2016-11-09 14:26:37,145  INFO  org.apache.spark.executor.Executor - logInfo: Fetching spark://192.168.100.103:49328/jars/mysql-connector-java-5.1.25.jar with timestamp 1478672791935
2016-11-09 14:26:37,146  INFO  org.apache.spark.util.Utils - logInfo: Fetching spark://192.168.100.103:49328/jars/mysql-connector-java-5.1.25.jar to /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/fetchFileTemp7974255327361208360.tmp
2016-11-09 14:26:37,151  INFO  org.apache.spark.executor.Executor - logInfo: Adding file:/tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc/userFiles-d269fc0c-1142-4a14-bb4b-8e243d6cd18e/mysql-connector-java-5.1.25.jar to class loader
2016-11-09 14:26:37,176  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,184  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-09 14:26:37,184  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-11-09 14:26:37,184  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-11-09 14:26:37,184  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-11-09 14:26:37,185  INFO  org.apache.hadoop.conf.Configuration.deprecation - warnOnceIfDeprecated: mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-11-09 14:26:37,259  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,259  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,413  INFO  org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate - logInfo: Code generated in 107.371989 ms
2016-11-09 14:26:37,436  INFO  org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - logInfo: Code generated in 12.850707 ms
2016-11-09 14:26:37,441  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_7_0 not found, computing it
2016-11-09 14:26:37,441  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,447  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,447  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,555  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_7_0 stored as values in memory (estimated size 74.1 KB, free 772.2 KB)
2016-11-09 14:26:37,555  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_7_0 in memory on localhost:30044 (size: 74.1 KB, free: 529.9 MB)
2016-11-09 14:26:37,599  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 0.0 (TID 0). 2866 bytes result sent to driver
2016-11-09 14:26:37,613  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 0.0 (TID 0) in 872 ms on localhost (1/1)
2016-11-09 14:26:37,615  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-11-09 14:26:37,617  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 0 (takeSample at KMeans.scala:378) finished in 0.885 s
2016-11-09 14:26:37,621  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 0 finished: takeSample at KMeans.scala:378, took 0.998808 s
2016-11-09 14:26:37,663  INFO  org.apache.spark.SparkContext - logInfo: Starting job: takeSample at KMeans.scala:378
2016-11-09 14:26:37,665  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 1 (takeSample at KMeans.scala:378) with 1 output partitions
2016-11-09 14:26:37,665  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 1 (takeSample at KMeans.scala:378)
2016-11-09 14:26:37,665  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:37,666  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:37,667  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 1 (PartitionwiseSampledRDD[11] at takeSample at KMeans.scala:378), which has no missing parents
2016-11-09 14:26:37,670  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_2 stored as values in memory (estimated size 30.5 KB, free 802.7 KB)
2016-11-09 14:26:37,677  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.4 KB, free 816.1 KB)
2016-11-09 14:26:37,678  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_2_piece0 in memory on localhost:30044 (size: 13.4 KB, free: 529.9 MB)
2016-11-09 14:26:37,678  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:37,679  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[11] at takeSample at KMeans.scala:378)
2016-11-09 14:26:37,679  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 1.0 with 1 tasks
2016-11-09 14:26:37,681  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,ANY, 3071 bytes)
2016-11-09 14:26:37,681  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 1.0 (TID 1)
2016-11-09 14:26:37,693  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,698  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,698  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,704  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:37,732  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 1.0 (TID 1). 3349 bytes result sent to driver
2016-11-09 14:26:37,771  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_1_piece0 on localhost:30044 in memory (size: 6.8 KB, free: 529.9 MB)
2016-11-09 14:26:37,772  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 1.0 (TID 1) in 92 ms on localhost (1/1)
2016-11-09 14:26:37,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 1 (takeSample at KMeans.scala:378) finished in 0.093 s
2016-11-09 14:26:37,772  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-11-09 14:26:37,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 1 finished: takeSample at KMeans.scala:378, took 0.108354 s
2016-11-09 14:26:37,773  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 5
2016-11-09 14:26:37,776  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_3 stored as values in memory (estimated size 1760.0 B, free 797.4 KB)
2016-11-09 14:26:37,781  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_3_piece0 stored as bytes in memory (estimated size 263.0 B, free 797.7 KB)
2016-11-09 14:26:37,781  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_3_piece0 in memory on localhost:30044 (size: 263.0 B, free: 529.9 MB)
2016-11-09 14:26:37,782  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 3 from broadcast at KMeans.scala:396
2016-11-09 14:26:37,797  INFO  org.apache.spark.SparkContext - logInfo: Starting job: aggregate at KMeans.scala:404
2016-11-09 14:26:37,798  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 2 (aggregate at KMeans.scala:404) with 1 output partitions
2016-11-09 14:26:37,798  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 2 (aggregate at KMeans.scala:404)
2016-11-09 14:26:37,798  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:37,799  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:37,799  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 2 (MapPartitionsRDD[13] at map at KMeans.scala:398), which has no missing parents
2016-11-09 14:26:37,803  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_4 stored as values in memory (estimated size 15.1 KB, free 812.8 KB)
2016-11-09 14:26:37,806  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.4 KB, free 820.2 KB)
2016-11-09 14:26:37,807  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_4_piece0 in memory on localhost:30044 (size: 7.4 KB, free: 529.9 MB)
2016-11-09 14:26:37,807  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:37,808  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at map at KMeans.scala:398)
2016-11-09 14:26:37,808  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 2.0 with 1 tasks
2016-11-09 14:26:37,809  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,ANY, 2994 bytes)
2016-11-09 14:26:37,809  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 2.0 (TID 2)
2016-11-09 14:26:37,815  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_13_0 not found, computing it
2016-11-09 14:26:37,815  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,820  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,820  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,826  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:37,826  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,830  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,830  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,835  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:37,850  WARN  com.github.fommil.netlib.BLAS - <clinit>: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2016-11-09 14:26:37,851  WARN  com.github.fommil.netlib.BLAS - <clinit>: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2016-11-09 14:26:37,907  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_13_0 stored as values in memory (estimated size 264.0 KB, free 1084.2 KB)
2016-11-09 14:26:37,908  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_13_0 in memory on localhost:30044 (size: 264.0 KB, free: 529.6 MB)
2016-11-09 14:26:37,915  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 2.0 (TID 2). 3004 bytes result sent to driver
2016-11-09 14:26:37,919  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 2.0 (TID 2) in 111 ms on localhost (1/1)
2016-11-09 14:26:37,919  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-11-09 14:26:37,919  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 2 (aggregate at KMeans.scala:404) finished in 0.111 s
2016-11-09 14:26:37,919  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 2 finished: aggregate at KMeans.scala:404, took 0.122618 s
2016-11-09 14:26:37,921  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 10 from persistence list
2016-11-09 14:26:37,924  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 10
2016-11-09 14:26:37,936  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at KMeans.scala:436
2016-11-09 14:26:37,937  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 3 (collect at KMeans.scala:436) with 1 output partitions
2016-11-09 14:26:37,938  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 3 (collect at KMeans.scala:436)
2016-11-09 14:26:37,938  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:37,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:37,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 3 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:428), which has no missing parents
2016-11-09 14:26:37,943  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_5 stored as values in memory (estimated size 15.2 KB, free 1099.4 KB)
2016-11-09 14:26:37,948  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1107.0 KB)
2016-11-09 14:26:37,948  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_5_piece0 in memory on localhost:30044 (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:37,949  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:37,949  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:428)
2016-11-09 14:26:37,949  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 3.0 with 1 tasks
2016-11-09 14:26:37,950  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,ANY, 3026 bytes)
2016-11-09 14:26:37,950  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 3.0 (TID 3)
2016-11-09 14:26:37,956  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:37,961  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:37,961  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:37,966  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:37,967  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_13_0 locally
2016-11-09 14:26:37,986  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 3.0 (TID 3). 4321 bytes result sent to driver
2016-11-09 14:26:37,994  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 3.0 (TID 3) in 44 ms on localhost (1/1)
2016-11-09 14:26:37,994  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-11-09 14:26:37,994  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 3 (collect at KMeans.scala:436) finished in 0.045 s
2016-11-09 14:26:37,995  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 3 finished: collect at KMeans.scala:436, took 0.058529 s
2016-11-09 14:26:37,997  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_6 stored as values in memory (estimated size 5.8 KB, free 1112.8 KB)
2016-11-09 14:26:38,003  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_6_piece0 stored as bytes in memory (estimated size 877.0 B, free 1113.7 KB)
2016-11-09 14:26:38,004  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_6_piece0 in memory on localhost:30044 (size: 877.0 B, free: 529.6 MB)
2016-11-09 14:26:38,004  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 6 from broadcast at KMeans.scala:396
2016-11-09 14:26:38,014  INFO  org.apache.spark.SparkContext - logInfo: Starting job: aggregate at KMeans.scala:404
2016-11-09 14:26:38,015  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 4 (aggregate at KMeans.scala:404) with 1 output partitions
2016-11-09 14:26:38,015  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 4 (aggregate at KMeans.scala:404)
2016-11-09 14:26:38,015  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,016  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,016  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 4 (MapPartitionsRDD[17] at map at KMeans.scala:398), which has no missing parents
2016-11-09 14:26:38,019  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_7 stored as values in memory (estimated size 15.3 KB, free 1129.0 KB)
2016-11-09 14:26:38,023  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1136.5 KB)
2016-11-09 14:26:38,024  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_7_piece0 in memory on localhost:30044 (size: 7.5 KB, free: 529.6 MB)
2016-11-09 14:26:38,025  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,025  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at map at KMeans.scala:398)
2016-11-09 14:26:38,025  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 4.0 with 1 tasks
2016-11-09 14:26:38,026  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,ANY, 3026 bytes)
2016-11-09 14:26:38,026  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 4.0 (TID 4)
2016-11-09 14:26:38,031  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_17_0 not found, computing it
2016-11-09 14:26:38,031  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,036  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,036  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,043  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,044  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_13_0 locally
2016-11-09 14:26:38,080  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_17_0 stored as values in memory (estimated size 264.0 KB, free 1400.5 KB)
2016-11-09 14:26:38,081  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_17_0 in memory on localhost:30044 (size: 264.0 KB, free: 529.3 MB)
2016-11-09 14:26:38,087  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 4.0 (TID 4). 3004 bytes result sent to driver
2016-11-09 14:26:38,091  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 4.0 (TID 4) in 64 ms on localhost (1/1)
2016-11-09 14:26:38,091  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,091  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 4 (aggregate at KMeans.scala:404) finished in 0.066 s
2016-11-09 14:26:38,091  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 4 finished: aggregate at KMeans.scala:404, took 0.077405 s
2016-11-09 14:26:38,093  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 13 from persistence list
2016-11-09 14:26:38,095  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 13
2016-11-09 14:26:38,105  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at KMeans.scala:436
2016-11-09 14:26:38,106  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 5 (collect at KMeans.scala:436) with 1 output partitions
2016-11-09 14:26:38,106  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 5 (collect at KMeans.scala:436)
2016-11-09 14:26:38,106  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,107  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,107  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 5 (MapPartitionsRDD[19] at mapPartitionsWithIndex at KMeans.scala:428), which has no missing parents
2016-11-09 14:26:38,110  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_8 stored as values in memory (estimated size 15.5 KB, free 1152.0 KB)
2016-11-09 14:26:38,115  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1159.6 KB)
2016-11-09 14:26:38,116  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_8_piece0 in memory on localhost:30044 (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,117  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,117  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at mapPartitionsWithIndex at KMeans.scala:428)
2016-11-09 14:26:38,117  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 5.0 with 1 tasks
2016-11-09 14:26:38,119  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,ANY, 3058 bytes)
2016-11-09 14:26:38,119  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 5.0 (TID 5)
2016-11-09 14:26:38,127  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,133  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,133  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,141  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,141  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_17_0 locally
2016-11-09 14:26:38,161  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 5.0 (TID 5). 5658 bytes result sent to driver
2016-11-09 14:26:38,168  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 5.0 (TID 5) in 50 ms on localhost (1/1)
2016-11-09 14:26:38,168  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,168  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 5 (collect at KMeans.scala:436) finished in 0.050 s
2016-11-09 14:26:38,169  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 5 finished: collect at KMeans.scala:436, took 0.063886 s
2016-11-09 14:26:38,170  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_9 stored as values in memory (estimated size 8.0 KB, free 1167.6 KB)
2016-11-09 14:26:38,175  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1214.0 B, free 1168.8 KB)
2016-11-09 14:26:38,176  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_9_piece0 in memory on localhost:30044 (size: 1214.0 B, free: 529.6 MB)
2016-11-09 14:26:38,178  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 9 from broadcast at KMeans.scala:396
2016-11-09 14:26:38,187  INFO  org.apache.spark.SparkContext - logInfo: Starting job: aggregate at KMeans.scala:404
2016-11-09 14:26:38,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 6 (aggregate at KMeans.scala:404) with 1 output partitions
2016-11-09 14:26:38,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 6 (aggregate at KMeans.scala:404)
2016-11-09 14:26:38,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,190  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,190  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 6 (MapPartitionsRDD[21] at map at KMeans.scala:398), which has no missing parents
2016-11-09 14:26:38,194  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_10 stored as values in memory (estimated size 15.5 KB, free 1184.4 KB)
2016-11-09 14:26:38,198  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1191.9 KB)
2016-11-09 14:26:38,199  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_10_piece0 in memory on localhost:30044 (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,199  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,200  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at map at KMeans.scala:398)
2016-11-09 14:26:38,200  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 6.0 with 1 tasks
2016-11-09 14:26:38,202  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,ANY, 3058 bytes)
2016-11-09 14:26:38,202  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 6.0 (TID 6)
2016-11-09 14:26:38,208  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_21_0 not found, computing it
2016-11-09 14:26:38,208  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,214  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,214  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,221  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,221  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_17_0 locally
2016-11-09 14:26:38,242  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_21_0 stored as values in memory (estimated size 264.0 KB, free 1455.9 KB)
2016-11-09 14:26:38,243  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_21_0 in memory on localhost:30044 (size: 264.0 KB, free: 529.3 MB)
2016-11-09 14:26:38,247  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 6.0 (TID 6). 3004 bytes result sent to driver
2016-11-09 14:26:38,250  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 6.0 (TID 6) in 50 ms on localhost (1/1)
2016-11-09 14:26:38,250  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,250  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 6 (aggregate at KMeans.scala:404) finished in 0.050 s
2016-11-09 14:26:38,251  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 6 finished: aggregate at KMeans.scala:404, took 0.063272 s
2016-11-09 14:26:38,252  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 17 from persistence list
2016-11-09 14:26:38,253  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 17
2016-11-09 14:26:38,262  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at KMeans.scala:436
2016-11-09 14:26:38,264  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 7 (collect at KMeans.scala:436) with 1 output partitions
2016-11-09 14:26:38,264  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 7 (collect at KMeans.scala:436)
2016-11-09 14:26:38,264  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,265  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,266  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 7 (MapPartitionsRDD[23] at mapPartitionsWithIndex at KMeans.scala:428), which has no missing parents
2016-11-09 14:26:38,269  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_11 stored as values in memory (estimated size 15.7 KB, free 1207.7 KB)
2016-11-09 14:26:38,275  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1215.4 KB)
2016-11-09 14:26:38,276  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_11_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.6 MB)
2016-11-09 14:26:38,277  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,277  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at mapPartitionsWithIndex at KMeans.scala:428)
2016-11-09 14:26:38,277  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 7.0 with 1 tasks
2016-11-09 14:26:38,279  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 7.0 (TID 7, localhost, partition 0,ANY, 3090 bytes)
2016-11-09 14:26:38,279  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 7.0 (TID 7)
2016-11-09 14:26:38,284  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,289  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,289  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,297  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,297  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_21_0 locally
2016-11-09 14:26:38,310  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 7.0 (TID 7). 5081 bytes result sent to driver
2016-11-09 14:26:38,317  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 7.0 (TID 7) in 38 ms on localhost (1/1)
2016-11-09 14:26:38,317  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,317  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 7 (collect at KMeans.scala:436) finished in 0.039 s
2016-11-09 14:26:38,317  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 7 finished: collect at KMeans.scala:436, took 0.054571 s
2016-11-09 14:26:38,319  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_12 stored as values in memory (estimated size 7.2 KB, free 1222.5 KB)
2016-11-09 14:26:38,323  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1086.0 B, free 1223.6 KB)
2016-11-09 14:26:38,324  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_12_piece0 in memory on localhost:30044 (size: 1086.0 B, free: 529.6 MB)
2016-11-09 14:26:38,325  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 12 from broadcast at KMeans.scala:396
2016-11-09 14:26:38,333  INFO  org.apache.spark.SparkContext - logInfo: Starting job: aggregate at KMeans.scala:404
2016-11-09 14:26:38,334  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 8 (aggregate at KMeans.scala:404) with 1 output partitions
2016-11-09 14:26:38,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 8 (aggregate at KMeans.scala:404)
2016-11-09 14:26:38,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,336  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,336  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 8 (MapPartitionsRDD[25] at map at KMeans.scala:398), which has no missing parents
2016-11-09 14:26:38,340  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_13 stored as values in memory (estimated size 15.8 KB, free 1239.4 KB)
2016-11-09 14:26:38,344  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1247.0 KB)
2016-11-09 14:26:38,344  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_13_piece0 in memory on localhost:30044 (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,345  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,345  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at map at KMeans.scala:398)
2016-11-09 14:26:38,345  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 8.0 with 1 tasks
2016-11-09 14:26:38,346  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 8.0 (TID 8, localhost, partition 0,ANY, 3090 bytes)
2016-11-09 14:26:38,346  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 8.0 (TID 8)
2016-11-09 14:26:38,352  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_25_0 not found, computing it
2016-11-09 14:26:38,352  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,358  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,358  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,364  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,364  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_21_0 locally
2016-11-09 14:26:38,384  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 9
2016-11-09 14:26:38,386  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_5_piece0 on localhost:30044 in memory (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,386  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 8
2016-11-09 14:26:38,387  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_4_piece0 on localhost:30044 in memory (size: 7.4 KB, free: 529.6 MB)
2016-11-09 14:26:38,388  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 7
2016-11-09 14:26:38,388  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_2_piece0 on localhost:30044 in memory (size: 13.4 KB, free: 529.6 MB)
2016-11-09 14:26:38,389  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 6
2016-11-09 14:26:38,389  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_11_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.6 MB)
2016-11-09 14:26:38,389  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 12
2016-11-09 14:26:38,390  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_10_piece0 on localhost:30044 in memory (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,390  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 11
2016-11-09 14:26:38,391  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_8_piece0 on localhost:30044 in memory (size: 7.6 KB, free: 529.6 MB)
2016-11-09 14:26:38,391  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 10
2016-11-09 14:26:38,392  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_7_piece0 on localhost:30044 in memory (size: 7.5 KB, free: 529.6 MB)
2016-11-09 14:26:38,393  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_25_0 stored as values in memory (estimated size 264.0 KB, free 1329.3 KB)
2016-11-09 14:26:38,393  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_25_0 in memory on localhost:30044 (size: 264.0 KB, free: 529.4 MB)
2016-11-09 14:26:38,396  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 8.0 (TID 8). 3004 bytes result sent to driver
2016-11-09 14:26:38,399  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 8.0 (TID 8) in 53 ms on localhost (1/1)
2016-11-09 14:26:38,399  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 8 (aggregate at KMeans.scala:404) finished in 0.054 s
2016-11-09 14:26:38,400  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 8.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,400  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 8 finished: aggregate at KMeans.scala:404, took 0.066485 s
2016-11-09 14:26:38,400  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 21 from persistence list
2016-11-09 14:26:38,401  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 21
2016-11-09 14:26:38,410  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at KMeans.scala:436
2016-11-09 14:26:38,411  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 9 (collect at KMeans.scala:436) with 1 output partitions
2016-11-09 14:26:38,411  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 9 (collect at KMeans.scala:436)
2016-11-09 14:26:38,411  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,412  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,412  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 9 (MapPartitionsRDD[27] at mapPartitionsWithIndex at KMeans.scala:428), which has no missing parents
2016-11-09 14:26:38,414  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_14 stored as values in memory (estimated size 15.9 KB, free 1081.3 KB)
2016-11-09 14:26:38,418  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.8 KB, free 1089.1 KB)
2016-11-09 14:26:38,419  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_14_piece0 in memory on localhost:30044 (size: 7.8 KB, free: 529.6 MB)
2016-11-09 14:26:38,419  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,419  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at mapPartitionsWithIndex at KMeans.scala:428)
2016-11-09 14:26:38,420  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 9.0 with 1 tasks
2016-11-09 14:26:38,421  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 9.0 (TID 9, localhost, partition 0,ANY, 3122 bytes)
2016-11-09 14:26:38,421  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 9.0 (TID 9)
2016-11-09 14:26:38,425  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,430  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,430  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,436  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,436  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_25_0 locally
2016-11-09 14:26:38,444  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 9.0 (TID 9). 5325 bytes result sent to driver
2016-11-09 14:26:38,452  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 9.0 (TID 9) in 32 ms on localhost (1/1)
2016-11-09 14:26:38,452  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 9 (collect at KMeans.scala:436) finished in 0.032 s
2016-11-09 14:26:38,452  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 9.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,453  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 9 finished: collect at KMeans.scala:436, took 0.042696 s
2016-11-09 14:26:38,454  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_15 stored as values in memory (estimated size 7.6 KB, free 1096.7 KB)
2016-11-09 14:26:38,458  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1155.0 B, free 1097.8 KB)
2016-11-09 14:26:38,459  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_15_piece0 in memory on localhost:30044 (size: 1155.0 B, free: 529.6 MB)
2016-11-09 14:26:38,459  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 15 from broadcast at KMeans.scala:396
2016-11-09 14:26:38,468  INFO  org.apache.spark.SparkContext - logInfo: Starting job: aggregate at KMeans.scala:404
2016-11-09 14:26:38,469  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 10 (aggregate at KMeans.scala:404) with 1 output partitions
2016-11-09 14:26:38,470  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 10 (aggregate at KMeans.scala:404)
2016-11-09 14:26:38,470  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,471  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,471  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 10 (MapPartitionsRDD[29] at map at KMeans.scala:398), which has no missing parents
2016-11-09 14:26:38,473  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_16 stored as values in memory (estimated size 16.0 KB, free 1113.8 KB)
2016-11-09 14:26:38,477  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1121.5 KB)
2016-11-09 14:26:38,477  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_16_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.6 MB)
2016-11-09 14:26:38,478  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,478  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at map at KMeans.scala:398)
2016-11-09 14:26:38,478  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 10.0 with 1 tasks
2016-11-09 14:26:38,479  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0,ANY, 3122 bytes)
2016-11-09 14:26:38,479  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 10.0 (TID 10)
2016-11-09 14:26:38,483  INFO  org.apache.spark.CacheManager - logInfo: Partition rdd_29_0 not found, computing it
2016-11-09 14:26:38,483  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,488  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,488  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,494  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,494  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_25_0 locally
2016-11-09 14:26:38,512  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block rdd_29_0 stored as values in memory (estimated size 264.0 KB, free 1385.5 KB)
2016-11-09 14:26:38,513  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added rdd_29_0 in memory on localhost:30044 (size: 264.0 KB, free: 529.4 MB)
2016-11-09 14:26:38,516  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 10.0 (TID 10). 3004 bytes result sent to driver
2016-11-09 14:26:38,519  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 10.0 (TID 10) in 40 ms on localhost (1/1)
2016-11-09 14:26:38,519  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 10 (aggregate at KMeans.scala:404) finished in 0.040 s
2016-11-09 14:26:38,519  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,519  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 10 finished: aggregate at KMeans.scala:404, took 0.050816 s
2016-11-09 14:26:38,520  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 25 from persistence list
2016-11-09 14:26:38,521  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 25
2016-11-09 14:26:38,529  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at KMeans.scala:436
2016-11-09 14:26:38,530  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 11 (collect at KMeans.scala:436) with 1 output partitions
2016-11-09 14:26:38,530  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 11 (collect at KMeans.scala:436)
2016-11-09 14:26:38,530  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:38,531  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:38,532  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 11 (MapPartitionsRDD[31] at mapPartitionsWithIndex at KMeans.scala:428), which has no missing parents
2016-11-09 14:26:38,533  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_17 stored as values in memory (estimated size 16.2 KB, free 1137.7 KB)
2016-11-09 14:26:38,538  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.8 KB, free 1145.5 KB)
2016-11-09 14:26:38,539  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_17_piece0 in memory on localhost:30044 (size: 7.8 KB, free: 529.6 MB)
2016-11-09 14:26:38,539  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,540  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at mapPartitionsWithIndex at KMeans.scala:428)
2016-11-09 14:26:38,540  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 11.0 with 1 tasks
2016-11-09 14:26:38,541  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 11.0 (TID 11, localhost, partition 0,ANY, 3154 bytes)
2016-11-09 14:26:38,541  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 11.0 (TID 11)
2016-11-09 14:26:38,545  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,549  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,550  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,555  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,555  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_29_0 locally
2016-11-09 14:26:38,562  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 11.0 (TID 11). 5346 bytes result sent to driver
2016-11-09 14:26:38,568  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 11.0 (TID 11) in 28 ms on localhost (1/1)
2016-11-09 14:26:38,569  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 11 (collect at KMeans.scala:436) finished in 0.028 s
2016-11-09 14:26:38,569  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,569  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 11 finished: collect at KMeans.scala:436, took 0.039247 s
2016-11-09 14:26:38,570  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 29 from persistence list
2016-11-09 14:26:38,570  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 29
2016-11-09 14:26:38,571  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_18 stored as values in memory (estimated size 33.9 KB, free 915.4 KB)
2016-11-09 14:26:38,576  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.9 KB, free 919.3 KB)
2016-11-09 14:26:38,576  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_18_piece0 in memory on localhost:30044 (size: 3.9 KB, free: 529.9 MB)
2016-11-09 14:26:38,577  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 18 from broadcast at KMeans.scala:450
2016-11-09 14:26:38,599  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:455
2016-11-09 14:26:38,607  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 32 (flatMap at KMeans.scala:451)
2016-11-09 14:26:38,608  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 12 (collectAsMap at KMeans.scala:455) with 1 output partitions
2016-11-09 14:26:38,608  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 13 (collectAsMap at KMeans.scala:455)
2016-11-09 14:26:38,608  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 12)
2016-11-09 14:26:38,608  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 12)
2016-11-09 14:26:38,609  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 12 (MapPartitionsRDD[32] at flatMap at KMeans.scala:451), which has no missing parents
2016-11-09 14:26:38,614  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_19 stored as values in memory (estimated size 15.2 KB, free 934.5 KB)
2016-11-09 14:26:38,617  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.6 KB, free 942.1 KB)
2016-11-09 14:26:38,618  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_19_piece0 in memory on localhost:30044 (size: 7.6 KB, free: 529.8 MB)
2016-11-09 14:26:38,618  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,620  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[32] at flatMap at KMeans.scala:451)
2016-11-09 14:26:38,620  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 12.0 with 1 tasks
2016-11-09 14:26:38,622  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 12.0 (TID 12, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:38,623  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 12.0 (TID 12)
2016-11-09 14:26:38,630  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,636  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,636  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,643  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,759  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 12.0 (TID 12). 2519 bytes result sent to driver
2016-11-09 14:26:38,765  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 12.0 (TID 12) in 144 ms on localhost (1/1)
2016-11-09 14:26:38,765  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 12.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,766  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 12 (flatMap at KMeans.scala:451) finished in 0.145 s
2016-11-09 14:26:38,767  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:38,767  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:38,767  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 13)
2016-11-09 14:26:38,768  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:38,769  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 13 (ShuffledRDD[33] at reduceByKey at KMeans.scala:455), which has no missing parents
2016-11-09 14:26:38,771  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_20 stored as values in memory (estimated size 2.6 KB, free 944.6 KB)
2016-11-09 14:26:38,774  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1554.0 B, free 946.2 KB)
2016-11-09 14:26:38,774  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_20_piece0 in memory on localhost:30044 (size: 1554.0 B, free: 529.8 MB)
2016-11-09 14:26:38,775  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,775  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 13 (ShuffledRDD[33] at reduceByKey at KMeans.scala:455)
2016-11-09 14:26:38,775  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 13.0 with 1 tasks
2016-11-09 14:26:38,778  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 13.0 (TID 13, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:38,779  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 13.0 (TID 13)
2016-11-09 14:26:38,788  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:38,789  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 3 ms
2016-11-09 14:26:38,833  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 13.0 (TID 13). 11972 bytes result sent to driver
2016-11-09 14:26:38,836  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 13.0 (TID 13) in 60 ms on localhost (1/1)
2016-11-09 14:26:38,836  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 13 (collectAsMap at KMeans.scala:455) finished in 0.060 s
2016-11-09 14:26:38,837  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 13.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,837  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 12 finished: collectAsMap at KMeans.scala:455, took 0.237769 s
2016-11-09 14:26:38,908  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 2 iterations.
2016-11-09 14:26:38,908  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 4 iterations.
2016-11-09 14:26:38,909  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 2 iterations.
2016-11-09 14:26:38,909  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 3 iterations.
2016-11-09 14:26:38,909  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 5 iterations.
2016-11-09 14:26:38,909  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 8 iterations.
2016-11-09 14:26:38,909  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 5 iterations.
2016-11-09 14:26:38,910  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 5 iterations.
2016-11-09 14:26:38,910  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 7 iterations.
2016-11-09 14:26:38,910  INFO  org.apache.spark.mllib.clustering.LocalKMeans - logInfo: Local KMeans++ converged in 8 iterations.
2016-11-09 14:26:38,920  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Initialization with k-means|| took 2.359 seconds.
2016-11-09 14:26:38,923  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_21 stored as values in memory (estimated size 3.6 KB, free 949.7 KB)
2016-11-09 14:26:38,926  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_21_piece0 stored as bytes in memory (estimated size 801.0 B, free 950.5 KB)
2016-11-09 14:26:38,927  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_21_piece0 in memory on localhost:30044 (size: 801.0 B, free: 529.8 MB)
2016-11-09 14:26:38,927  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 21 from broadcast at KMeans.scala:276
2016-11-09 14:26:38,938  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:38,938  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 34 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:38,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 13 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:38,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 15 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:38,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 14)
2016-11-09 14:26:38,939  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 14)
2016-11-09 14:26:38,940  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 14 (MapPartitionsRDD[34] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:38,941  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_22 stored as values in memory (estimated size 15.7 KB, free 966.2 KB)
2016-11-09 14:26:38,945  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.7 KB, free 974.0 KB)
2016-11-09 14:26:38,945  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_22_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:38,946  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,946  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[34] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:38,946  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 14.0 with 1 tasks
2016-11-09 14:26:38,947  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:38,947  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 14.0 (TID 14)
2016-11-09 14:26:38,951  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:38,956  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:38,956  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:38,962  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:38,985  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 14.0 (TID 14). 2798 bytes result sent to driver
2016-11-09 14:26:38,987  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 14.0 (TID 14) in 41 ms on localhost (1/1)
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 14 (mapPartitions at KMeans.scala:279) finished in 0.042 s
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 15)
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:38,988  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 15 (ShuffledRDD[35] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:38,989  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_23 stored as values in memory (estimated size 2.9 KB, free 976.9 KB)
2016-11-09 14:26:38,992  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_23_piece0 stored as bytes in memory (estimated size 1660.0 B, free 978.5 KB)
2016-11-09 14:26:38,993  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_23_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:38,993  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 23 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:38,993  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[35] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:38,993  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 15.0 with 1 tasks
2016-11-09 14:26:38,994  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 15.0 (TID 15, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:38,994  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 15.0 (TID 15)
2016-11-09 14:26:38,996  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:38,996  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,005  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 15.0 (TID 15). 2647 bytes result sent to driver
2016-11-09 14:26:39,009  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 15.0 (TID 15) in 14 ms on localhost (1/1)
2016-11-09 14:26:39,009  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 15 (collectAsMap at KMeans.scala:302) finished in 0.015 s
2016-11-09 14:26:39,009  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 15.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,009  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 13 finished: collectAsMap at KMeans.scala:302, took 0.071203 s
2016-11-09 14:26:39,012  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_24 stored as values in memory (estimated size 3.6 KB, free 982.1 KB)
2016-11-09 14:26:39,014  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_24_piece0 stored as bytes in memory (estimated size 685.0 B, free 982.8 KB)
2016-11-09 14:26:39,014  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_24_piece0 in memory on localhost:30044 (size: 685.0 B, free: 529.8 MB)
2016-11-09 14:26:39,015  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 24 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,022  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,022  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 36 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,022  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 14 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,022  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 17 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,023  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 16)
2016-11-09 14:26:39,023  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 16)
2016-11-09 14:26:39,023  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 16 (MapPartitionsRDD[36] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,025  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_25 stored as values in memory (estimated size 15.7 KB, free 998.5 KB)
2016-11-09 14:26:39,028  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1006.2 KB)
2016-11-09 14:26:39,028  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_25_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,028  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 25 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,029  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[36] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,029  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 16.0 with 1 tasks
2016-11-09 14:26:39,030  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 16.0 (TID 16, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,030  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 16.0 (TID 16)
2016-11-09 14:26:39,034  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,038  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,038  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,043  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,057  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 16.0 (TID 16). 2798 bytes result sent to driver
2016-11-09 14:26:39,059  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 16.0 (TID 16) in 30 ms on localhost (1/1)
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 16.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 16 (mapPartitions at KMeans.scala:279) finished in 0.031 s
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 17)
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,060  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 17 (ShuffledRDD[37] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,061  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_26 stored as values in memory (estimated size 2.9 KB, free 1009.1 KB)
2016-11-09 14:26:39,064  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_26_piece0 stored as bytes in memory (estimated size 1660.0 B, free 1010.7 KB)
2016-11-09 14:26:39,064  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_26_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,065  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,065  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[37] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,065  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 17.0 with 1 tasks
2016-11-09 14:26:39,065  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 17.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,066  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 17.0 (TID 17)
2016-11-09 14:26:39,067  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,067  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,074  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 17.0 (TID 17). 2647 bytes result sent to driver
2016-11-09 14:26:39,078  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 17.0 (TID 17) in 12 ms on localhost (1/1)
2016-11-09 14:26:39,078  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 17.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,078  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 17 (collectAsMap at KMeans.scala:302) finished in 0.013 s
2016-11-09 14:26:39,078  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 14 finished: collectAsMap at KMeans.scala:302, took 0.056457 s
2016-11-09 14:26:39,080  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 6 finished in 2 iterations
2016-11-09 14:26:39,081  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_27 stored as values in memory (estimated size 3.2 KB, free 1014.0 KB)
2016-11-09 14:26:39,084  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_27_piece0 stored as bytes in memory (estimated size 657.0 B, free 1014.6 KB)
2016-11-09 14:26:39,084  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_27_piece0 in memory on localhost:30044 (size: 657.0 B, free: 529.8 MB)
2016-11-09 14:26:39,085  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 27 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,092  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,092  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 38 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,093  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 15 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,093  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 19 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,093  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 18)
2016-11-09 14:26:39,093  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 18)
2016-11-09 14:26:39,093  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 18 (MapPartitionsRDD[38] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,095  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_28 stored as values in memory (estimated size 15.7 KB, free 1030.3 KB)
2016-11-09 14:26:39,098  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_28_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1038.0 KB)
2016-11-09 14:26:39,098  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_28_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,099  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,099  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[38] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,099  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 18.0 with 1 tasks
2016-11-09 14:26:39,100  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 18.0 (TID 18, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,101  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 18.0 (TID 18)
2016-11-09 14:26:39,106  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,111  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,111  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,117  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,129  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 18.0 (TID 18). 2774 bytes result sent to driver
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 18.0 (TID 18) in 32 ms on localhost (1/1)
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 18.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 18 (mapPartitions at KMeans.scala:279) finished in 0.033 s
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 19)
2016-11-09 14:26:39,132  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,133  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 19 (ShuffledRDD[39] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,134  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_29 stored as values in memory (estimated size 2.9 KB, free 1040.9 KB)
2016-11-09 14:26:39,149  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 25
2016-11-09 14:26:39,150  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 25
2016-11-09 14:26:39,151  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_12_piece0 on localhost:30044 in memory (size: 1086.0 B, free: 529.8 MB)
2016-11-09 14:26:39,151  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_29_piece0 stored as bytes in memory (estimated size 1660.0 B, free 1041.5 KB)
2016-11-09 14:26:39,151  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_29_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,152  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 21
2016-11-09 14:26:39,152  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,152  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 21
2016-11-09 14:26:39,152  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[39] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,152  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 19.0 with 1 tasks
2016-11-09 14:26:39,153  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_9_piece0 on localhost:30044 in memory (size: 1214.0 B, free: 529.8 MB)
2016-11-09 14:26:39,153  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 35
2016-11-09 14:26:39,153  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 19.0 (TID 19, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,153  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 34
2016-11-09 14:26:39,153  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 33
2016-11-09 14:26:39,153  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 19.0 (TID 19)
2016-11-09 14:26:39,153  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 32
2016-11-09 14:26:39,153  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 31
2016-11-09 14:26:39,154  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_23_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,154  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 30
2016-11-09 14:26:39,155  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,155  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_22_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,155  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,155  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 29
2016-11-09 14:26:39,157  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 1
2016-11-09 14:26:39,158  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_21_piece0 on localhost:30044 in memory (size: 801.0 B, free: 529.8 MB)
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 28
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 27
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 26
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 25
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 24
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 23
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 22
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 21
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 20
2016-11-09 14:26:39,159  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 19
2016-11-09 14:26:39,160  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_20_piece0 on localhost:30044 in memory (size: 1554.0 B, free: 529.8 MB)
2016-11-09 14:26:39,160  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 18
2016-11-09 14:26:39,160  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_19_piece0 on localhost:30044 in memory (size: 7.6 KB, free: 529.8 MB)
2016-11-09 14:26:39,161  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 17
2016-11-09 14:26:39,161  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 0
2016-11-09 14:26:39,162  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_18_piece0 on localhost:30044 in memory (size: 3.9 KB, free: 529.8 MB)
2016-11-09 14:26:39,162  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_17_piece0 on localhost:30044 in memory (size: 7.8 KB, free: 529.8 MB)
2016-11-09 14:26:39,162  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 16
2016-11-09 14:26:39,163  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_16_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,163  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 15
2016-11-09 14:26:39,164  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 29
2016-11-09 14:26:39,164  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 29
2016-11-09 14:26:39,166  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 19.0 (TID 19). 2500 bytes result sent to driver
2016-11-09 14:26:39,166  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_15_piece0 on localhost:30044 in memory (size: 1155.0 B, free: 529.9 MB)
2016-11-09 14:26:39,167  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_14_piece0 on localhost:30044 in memory (size: 7.8 KB, free: 529.9 MB)
2016-11-09 14:26:39,167  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 14
2016-11-09 14:26:39,168  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_13_piece0 on localhost:30044 in memory (size: 7.6 KB, free: 529.9 MB)
2016-11-09 14:26:39,168  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 13
2016-11-09 14:26:39,168  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 37
2016-11-09 14:26:39,168  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 36
2016-11-09 14:26:39,168  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 17
2016-11-09 14:26:39,169  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 17
2016-11-09 14:26:39,169  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_6_piece0 on localhost:30044 in memory (size: 877.0 B, free: 529.9 MB)
2016-11-09 14:26:39,169  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 19.0 (TID 19) in 16 ms on localhost (1/1)
2016-11-09 14:26:39,170  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 19 (collectAsMap at KMeans.scala:302) finished in 0.016 s
2016-11-09 14:26:39,170  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 19.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,170  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 13
2016-11-09 14:26:39,170  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 15 finished: collectAsMap at KMeans.scala:302, took 0.078012 s
2016-11-09 14:26:39,170  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 13
2016-11-09 14:26:39,171  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_3_piece0 on localhost:30044 in memory (size: 263.0 B, free: 529.9 MB)
2016-11-09 14:26:39,171  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 2 finished in 3 iterations
2016-11-09 14:26:39,171  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_26_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,172  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_30 stored as values in memory (estimated size 2.9 KB, free 814.2 KB)
2016-11-09 14:26:39,172  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 42
2016-11-09 14:26:39,173  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_25_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,173  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 41
2016-11-09 14:26:39,173  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 2
2016-11-09 14:26:39,174  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_24_piece0 on localhost:30044 in memory (size: 685.0 B, free: 529.9 MB)
2016-11-09 14:26:39,174  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 40
2016-11-09 14:26:39,174  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 39
2016-11-09 14:26:39,174  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 38
2016-11-09 14:26:39,175  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_30_piece0 stored as bytes in memory (estimated size 558.0 B, free 787.1 KB)
2016-11-09 14:26:39,176  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_30_piece0 in memory on localhost:30044 (size: 558.0 B, free: 529.9 MB)
2016-11-09 14:26:39,176  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 30 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,183  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,183  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 40 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,184  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 16 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,184  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 21 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,184  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 20)
2016-11-09 14:26:39,184  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 20)
2016-11-09 14:26:39,184  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 20 (MapPartitionsRDD[40] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,186  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_31 stored as values in memory (estimated size 15.6 KB, free 802.7 KB)
2016-11-09 14:26:39,189  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.7 KB, free 810.4 KB)
2016-11-09 14:26:39,189  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_31_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,190  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,190  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[40] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,190  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 20.0 with 1 tasks
2016-11-09 14:26:39,191  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 20.0 (TID 20, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,191  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 20.0 (TID 20)
2016-11-09 14:26:39,195  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,199  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,199  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,203  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,214  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 20.0 (TID 20). 2750 bytes result sent to driver
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 20.0 (TID 20) in 26 ms on localhost (1/1)
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 20.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 20 (mapPartitions at KMeans.scala:279) finished in 0.026 s
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 21)
2016-11-09 14:26:39,216  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,217  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 21 (ShuffledRDD[41] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,217  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_32 stored as values in memory (estimated size 2.9 KB, free 813.3 KB)
2016-11-09 14:26:39,219  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1660.0 B, free 815.0 KB)
2016-11-09 14:26:39,220  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_32_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,220  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,220  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[41] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,221  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 21.0 with 1 tasks
2016-11-09 14:26:39,221  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 21.0 (TID 21, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,221  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 21.0 (TID 21)
2016-11-09 14:26:39,223  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,223  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,229  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 21.0 (TID 21). 2350 bytes result sent to driver
2016-11-09 14:26:39,231  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 21.0 (TID 21) in 10 ms on localhost (1/1)
2016-11-09 14:26:39,231  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 21.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,231  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 21 (collectAsMap at KMeans.scala:302) finished in 0.010 s
2016-11-09 14:26:39,232  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 16 finished: collectAsMap at KMeans.scala:302, took 0.048626 s
2016-11-09 14:26:39,232  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 1 finished in 4 iterations
2016-11-09 14:26:39,233  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_33 stored as values in memory (estimated size 2.5 KB, free 817.5 KB)
2016-11-09 14:26:39,235  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_33_piece0 stored as bytes in memory (estimated size 525.0 B, free 818.0 KB)
2016-11-09 14:26:39,236  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_33_piece0 in memory on localhost:30044 (size: 525.0 B, free: 529.9 MB)
2016-11-09 14:26:39,236  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 33 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,243  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,243  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 42 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,244  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 17 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,244  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 23 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,244  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 22)
2016-11-09 14:26:39,244  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 22)
2016-11-09 14:26:39,244  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 22 (MapPartitionsRDD[42] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,246  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_34 stored as values in memory (estimated size 15.6 KB, free 833.6 KB)
2016-11-09 14:26:39,248  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_34_piece0 stored as bytes in memory (estimated size 7.7 KB, free 841.3 KB)
2016-11-09 14:26:39,248  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_34_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,248  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,248  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[42] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,249  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 22.0 with 1 tasks
2016-11-09 14:26:39,249  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 22.0 (TID 22, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,249  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 22.0 (TID 22)
2016-11-09 14:26:39,255  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,260  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,260  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,265  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,275  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 22.0 (TID 22). 2726 bytes result sent to driver
2016-11-09 14:26:39,277  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 22.0 (TID 22) in 28 ms on localhost (1/1)
2016-11-09 14:26:39,277  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 22.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,277  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 22 (mapPartitions at KMeans.scala:279) finished in 0.028 s
2016-11-09 14:26:39,277  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,277  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,278  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 23)
2016-11-09 14:26:39,278  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,278  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 23 (ShuffledRDD[43] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,279  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_35 stored as values in memory (estimated size 2.9 KB, free 844.2 KB)
2016-11-09 14:26:39,281  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_35_piece0 stored as bytes in memory (estimated size 1655.0 B, free 845.9 KB)
2016-11-09 14:26:39,281  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_35_piece0 in memory on localhost:30044 (size: 1655.0 B, free: 529.9 MB)
2016-11-09 14:26:39,282  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,282  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 23 (ShuffledRDD[43] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,282  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 23.0 with 1 tasks
2016-11-09 14:26:39,283  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 23.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,283  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 23.0 (TID 23)
2016-11-09 14:26:39,284  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,284  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,289  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 23.0 (TID 23). 2203 bytes result sent to driver
2016-11-09 14:26:39,292  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 23.0 (TID 23) in 8 ms on localhost (1/1)
2016-11-09 14:26:39,292  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 23 (collectAsMap at KMeans.scala:302) finished in 0.010 s
2016-11-09 14:26:39,292  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 23.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,292  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 17 finished: collectAsMap at KMeans.scala:302, took 0.048902 s
2016-11-09 14:26:39,293  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 4 finished in 5 iterations
2016-11-09 14:26:39,293  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_36 stored as values in memory (estimated size 2.2 KB, free 848.0 KB)
2016-11-09 14:26:39,295  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_36_piece0 stored as bytes in memory (estimated size 462.0 B, free 848.5 KB)
2016-11-09 14:26:39,295  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_36_piece0 in memory on localhost:30044 (size: 462.0 B, free: 529.9 MB)
2016-11-09 14:26:39,296  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 36 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,302  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,303  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 44 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,303  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 18 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,303  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 25 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,303  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 24)
2016-11-09 14:26:39,304  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 24)
2016-11-09 14:26:39,304  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 24 (MapPartitionsRDD[44] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,305  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_37 stored as values in memory (estimated size 15.5 KB, free 864.0 KB)
2016-11-09 14:26:39,308  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_37_piece0 stored as bytes in memory (estimated size 7.7 KB, free 871.8 KB)
2016-11-09 14:26:39,308  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_37_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,308  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 37 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,308  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[44] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,308  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 24.0 with 1 tasks
2016-11-09 14:26:39,309  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 24.0 (TID 24, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,309  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 24.0 (TID 24)
2016-11-09 14:26:39,313  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,317  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,318  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,323  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,333  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 24.0 (TID 24). 2702 bytes result sent to driver
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 24.0 (TID 24) in 26 ms on localhost (1/1)
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 24.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 24 (mapPartitions at KMeans.scala:279) finished in 0.026 s
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 25)
2016-11-09 14:26:39,335  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,336  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 25 (ShuffledRDD[45] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,336  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_38 stored as values in memory (estimated size 2.9 KB, free 874.7 KB)
2016-11-09 14:26:39,339  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_38_piece0 stored as bytes in memory (estimated size 1660.0 B, free 876.3 KB)
2016-11-09 14:26:39,339  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_38_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,339  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 38 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,340  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[45] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,340  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 25.0 with 1 tasks
2016-11-09 14:26:39,340  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 25.0 (TID 25, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,341  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 25.0 (TID 25)
2016-11-09 14:26:39,342  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,342  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,347  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 25.0 (TID 25). 2054 bytes result sent to driver
2016-11-09 14:26:39,349  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 25.0 (TID 25) in 9 ms on localhost (1/1)
2016-11-09 14:26:39,349  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 25 (collectAsMap at KMeans.scala:302) finished in 0.009 s
2016-11-09 14:26:39,349  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 25.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,350  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 18 finished: collectAsMap at KMeans.scala:302, took 0.047072 s
2016-11-09 14:26:39,350  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 8 finished in 6 iterations
2016-11-09 14:26:39,350  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 9 finished in 6 iterations
2016-11-09 14:26:39,351  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_39 stored as values in memory (estimated size 1496.0 B, free 877.7 KB)
2016-11-09 14:26:39,353  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_39_piece0 stored as bytes in memory (estimated size 348.0 B, free 878.1 KB)
2016-11-09 14:26:39,353  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_39_piece0 in memory on localhost:30044 (size: 348.0 B, free: 529.9 MB)
2016-11-09 14:26:39,354  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 39 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,360  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,362  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 46 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,362  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 19 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,362  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 27 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,362  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 26)
2016-11-09 14:26:39,362  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 26)
2016-11-09 14:26:39,363  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 26 (MapPartitionsRDD[46] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,364  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_40 stored as values in memory (estimated size 15.5 KB, free 893.5 KB)
2016-11-09 14:26:39,366  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_40_piece0 stored as bytes in memory (estimated size 7.7 KB, free 901.3 KB)
2016-11-09 14:26:39,367  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_40_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,367  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 40 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,368  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[46] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,368  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 26.0 with 1 tasks
2016-11-09 14:26:39,368  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 26.0 (TID 26, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,369  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 26.0 (TID 26)
2016-11-09 14:26:39,372  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,376  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,376  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,382  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,392  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 26.0 (TID 26). 2654 bytes result sent to driver
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 26.0 (TID 26) in 25 ms on localhost (1/1)
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 26.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 26 (mapPartitions at KMeans.scala:279) finished in 0.026 s
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 27)
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,394  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 27 (ShuffledRDD[47] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,395  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_41 stored as values in memory (estimated size 2.9 KB, free 904.2 KB)
2016-11-09 14:26:39,397  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1660.0 B, free 905.8 KB)
2016-11-09 14:26:39,398  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_41_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,398  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 41 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,398  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[47] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,398  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 27.0 with 1 tasks
2016-11-09 14:26:39,399  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 27.0 (TID 27, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,399  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 27.0 (TID 27)
2016-11-09 14:26:39,400  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,400  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,406  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 27.0 (TID 27). 1760 bytes result sent to driver
2016-11-09 14:26:39,408  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 27.0 (TID 27) in 10 ms on localhost (1/1)
2016-11-09 14:26:39,409  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 27 (collectAsMap at KMeans.scala:302) finished in 0.011 s
2016-11-09 14:26:39,409  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 27.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,409  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 19 finished: collectAsMap at KMeans.scala:302, took 0.048967 s
2016-11-09 14:26:39,410  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_42 stored as values in memory (estimated size 1496.0 B, free 907.3 KB)
2016-11-09 14:26:39,412  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_42_piece0 stored as bytes in memory (estimated size 354.0 B, free 907.6 KB)
2016-11-09 14:26:39,412  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_42_piece0 in memory on localhost:30044 (size: 354.0 B, free: 529.8 MB)
2016-11-09 14:26:39,412  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 42 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,419  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,419  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 48 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,420  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 20 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,420  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 29 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,420  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 28)
2016-11-09 14:26:39,420  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 28)
2016-11-09 14:26:39,421  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 28 (MapPartitionsRDD[48] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,422  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_43 stored as values in memory (estimated size 15.5 KB, free 923.1 KB)
2016-11-09 14:26:39,424  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_43_piece0 stored as bytes in memory (estimated size 7.7 KB, free 930.8 KB)
2016-11-09 14:26:39,424  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_43_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,425  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 43 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,425  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[48] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,425  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 28.0 with 1 tasks
2016-11-09 14:26:39,425  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 28.0 (TID 28, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,426  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 28.0 (TID 28)
2016-11-09 14:26:39,429  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,433  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,433  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,437  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,446  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 28.0 (TID 28). 2654 bytes result sent to driver
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 28.0 (TID 28) in 22 ms on localhost (1/1)
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 28.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 28 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 29)
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,448  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 29 (ShuffledRDD[49] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,449  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_44 stored as values in memory (estimated size 2.9 KB, free 933.7 KB)
2016-11-09 14:26:39,451  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_44_piece0 stored as bytes in memory (estimated size 1660.0 B, free 935.3 KB)
2016-11-09 14:26:39,451  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_44_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,452  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 44 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,452  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 29 (ShuffledRDD[49] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,452  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 29.0 with 1 tasks
2016-11-09 14:26:39,452  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 29.0 (TID 29, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,453  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 29.0 (TID 29)
2016-11-09 14:26:39,454  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,454  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,459  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 29.0 (TID 29). 1760 bytes result sent to driver
2016-11-09 14:26:39,461  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 29.0 (TID 29) in 9 ms on localhost (1/1)
2016-11-09 14:26:39,461  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 29 (collectAsMap at KMeans.scala:302) finished in 0.009 s
2016-11-09 14:26:39,461  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 29.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,462  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 20 finished: collectAsMap at KMeans.scala:302, took 0.042502 s
2016-11-09 14:26:39,463  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_45 stored as values in memory (estimated size 1496.0 B, free 936.8 KB)
2016-11-09 14:26:39,465  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_45_piece0 stored as bytes in memory (estimated size 353.0 B, free 937.1 KB)
2016-11-09 14:26:39,465  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_45_piece0 in memory on localhost:30044 (size: 353.0 B, free: 529.8 MB)
2016-11-09 14:26:39,465  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 45 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,472  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,473  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 50 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,473  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 21 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,473  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 31 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,473  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 30)
2016-11-09 14:26:39,473  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 30)
2016-11-09 14:26:39,474  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 30 (MapPartitionsRDD[50] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,475  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_46 stored as values in memory (estimated size 15.5 KB, free 952.6 KB)
2016-11-09 14:26:39,477  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_46_piece0 stored as bytes in memory (estimated size 7.7 KB, free 960.3 KB)
2016-11-09 14:26:39,477  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_46_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,478  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 46 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,478  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[50] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,478  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 30.0 with 1 tasks
2016-11-09 14:26:39,478  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 30.0 (TID 30, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,479  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 30.0 (TID 30)
2016-11-09 14:26:39,481  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,486  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,486  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,491  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,498  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 30.0 (TID 30). 2654 bytes result sent to driver
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 30.0 (TID 30) in 22 ms on localhost (1/1)
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 30 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 30.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 31)
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,501  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 31 (ShuffledRDD[51] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,502  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_47 stored as values in memory (estimated size 2.9 KB, free 963.2 KB)
2016-11-09 14:26:39,504  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_47_piece0 stored as bytes in memory (estimated size 1660.0 B, free 964.8 KB)
2016-11-09 14:26:39,504  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_47_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,504  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 47 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,505  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 31 (ShuffledRDD[51] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,505  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 31.0 with 1 tasks
2016-11-09 14:26:39,505  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 31.0 (TID 31, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,506  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 31.0 (TID 31)
2016-11-09 14:26:39,507  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,507  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,512  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 31.0 (TID 31). 1760 bytes result sent to driver
2016-11-09 14:26:39,515  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 31.0 (TID 31) in 10 ms on localhost (1/1)
2016-11-09 14:26:39,515  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 31 (collectAsMap at KMeans.scala:302) finished in 0.010 s
2016-11-09 14:26:39,515  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 31.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,516  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 21 finished: collectAsMap at KMeans.scala:302, took 0.043723 s
2016-11-09 14:26:39,517  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_48 stored as values in memory (estimated size 1496.0 B, free 966.3 KB)
2016-11-09 14:26:39,519  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_48_piece0 stored as bytes in memory (estimated size 335.0 B, free 966.6 KB)
2016-11-09 14:26:39,520  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_48_piece0 in memory on localhost:30044 (size: 335.0 B, free: 529.8 MB)
2016-11-09 14:26:39,520  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 48 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,527  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,528  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 52 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,528  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 22 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,528  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 33 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,529  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 32)
2016-11-09 14:26:39,529  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 32)
2016-11-09 14:26:39,529  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 32 (MapPartitionsRDD[52] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,531  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_49 stored as values in memory (estimated size 15.5 KB, free 982.1 KB)
2016-11-09 14:26:39,533  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_49_piece0 stored as bytes in memory (estimated size 7.7 KB, free 989.8 KB)
2016-11-09 14:26:39,533  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_49_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,534  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 49 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,534  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[52] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,534  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 32.0 with 1 tasks
2016-11-09 14:26:39,535  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 32.0 (TID 32, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,535  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 32.0 (TID 32)
2016-11-09 14:26:39,538  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,542  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,542  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,546  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,554  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 32.0 (TID 32). 2654 bytes result sent to driver
2016-11-09 14:26:39,557  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 32.0 (TID 32) in 23 ms on localhost (1/1)
2016-11-09 14:26:39,557  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 32 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:39,557  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 32.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,557  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,558  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,558  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 33)
2016-11-09 14:26:39,558  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,558  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 33 (ShuffledRDD[53] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,559  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_50 stored as values in memory (estimated size 2.9 KB, free 992.7 KB)
2016-11-09 14:26:39,561  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1660.0 B, free 994.3 KB)
2016-11-09 14:26:39,562  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_50_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,562  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 50 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,562  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[53] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,562  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 33.0 with 1 tasks
2016-11-09 14:26:39,563  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 33.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,563  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 33.0 (TID 33)
2016-11-09 14:26:39,564  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,564  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,569  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 33.0 (TID 33). 1760 bytes result sent to driver
2016-11-09 14:26:39,573  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 33.0 (TID 33) in 9 ms on localhost (1/1)
2016-11-09 14:26:39,573  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 33 (collectAsMap at KMeans.scala:302) finished in 0.010 s
2016-11-09 14:26:39,573  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 33.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,573  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 22 finished: collectAsMap at KMeans.scala:302, took 0.045560 s
2016-11-09 14:26:39,574  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_51 stored as values in memory (estimated size 1496.0 B, free 995.8 KB)
2016-11-09 14:26:39,577  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_51_piece0 stored as bytes in memory (estimated size 339.0 B, free 996.1 KB)
2016-11-09 14:26:39,577  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_51_piece0 in memory on localhost:30044 (size: 339.0 B, free: 529.8 MB)
2016-11-09 14:26:39,577  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 51 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,584  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,584  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 54 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,585  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 23 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,585  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 35 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,585  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 34)
2016-11-09 14:26:39,585  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 34)
2016-11-09 14:26:39,585  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 34 (MapPartitionsRDD[54] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,587  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_52 stored as values in memory (estimated size 15.5 KB, free 1011.6 KB)
2016-11-09 14:26:39,589  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_52_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1019.3 KB)
2016-11-09 14:26:39,589  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_52_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,589  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 52 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,590  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[54] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,590  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 34.0 with 1 tasks
2016-11-09 14:26:39,590  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 34.0 (TID 34, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,591  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 34.0 (TID 34)
2016-11-09 14:26:39,594  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,598  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,598  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,603  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,611  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 34.0 (TID 34). 2654 bytes result sent to driver
2016-11-09 14:26:39,614  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 34.0 (TID 34) in 24 ms on localhost (1/1)
2016-11-09 14:26:39,614  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 34 (mapPartitions at KMeans.scala:279) finished in 0.024 s
2016-11-09 14:26:39,614  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 34.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,614  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,614  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,615  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 35)
2016-11-09 14:26:39,615  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,615  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 35 (ShuffledRDD[55] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,616  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_53 stored as values in memory (estimated size 2.9 KB, free 1022.2 KB)
2016-11-09 14:26:39,618  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_53_piece0 stored as bytes in memory (estimated size 1660.0 B, free 1023.8 KB)
2016-11-09 14:26:39,618  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_53_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,618  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 53 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,619  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 35 (ShuffledRDD[55] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,619  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 35.0 with 1 tasks
2016-11-09 14:26:39,619  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 35.0 (TID 35, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,620  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 35.0 (TID 35)
2016-11-09 14:26:39,621  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,621  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,625  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 35.0 (TID 35). 1760 bytes result sent to driver
2016-11-09 14:26:39,628  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 35.0 (TID 35) in 9 ms on localhost (1/1)
2016-11-09 14:26:39,628  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 35 (collectAsMap at KMeans.scala:302) finished in 0.009 s
2016-11-09 14:26:39,628  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 35.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,628  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 23 finished: collectAsMap at KMeans.scala:302, took 0.044029 s
2016-11-09 14:26:39,629  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_54 stored as values in memory (estimated size 1496.0 B, free 1025.3 KB)
2016-11-09 14:26:39,631  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_54_piece0 stored as bytes in memory (estimated size 332.0 B, free 1025.6 KB)
2016-11-09 14:26:39,632  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_54_piece0 in memory on localhost:30044 (size: 332.0 B, free: 529.8 MB)
2016-11-09 14:26:39,632  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 54 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,639  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,639  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 56 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,640  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 24 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,640  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 37 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,640  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 36)
2016-11-09 14:26:39,640  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 36)
2016-11-09 14:26:39,640  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 36 (MapPartitionsRDD[56] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,642  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_55 stored as values in memory (estimated size 15.5 KB, free 1041.1 KB)
2016-11-09 14:26:39,663  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_42_piece0 on localhost:30044 in memory (size: 354.0 B, free: 529.8 MB)
2016-11-09 14:26:39,664  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 75
2016-11-09 14:26:39,664  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 74
2016-11-09 14:26:39,664  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 73
2016-11-09 14:26:39,665  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_35_piece0 on localhost:30044 in memory (size: 1655.0 B, free: 529.8 MB)
2016-11-09 14:26:39,665  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 72
2016-11-09 14:26:39,665  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_55_piece0 stored as bytes in memory (estimated size 7.7 KB, free 1042.5 KB)
2016-11-09 14:26:39,665  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_55_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,666  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_34_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,666  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 55 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,666  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 71
2016-11-09 14:26:39,666  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[56] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,666  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 36.0 with 1 tasks
2016-11-09 14:26:39,666  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 5
2016-11-09 14:26:39,667  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_33_piece0 on localhost:30044 in memory (size: 525.0 B, free: 529.8 MB)
2016-11-09 14:26:39,667  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 70
2016-11-09 14:26:39,667  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 36.0 (TID 36, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,667  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 69
2016-11-09 14:26:39,667  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 68
2016-11-09 14:26:39,667  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 36.0 (TID 36)
2016-11-09 14:26:39,667  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 67
2016-11-09 14:26:39,667  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 66
2016-11-09 14:26:39,668  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 65
2016-11-09 14:26:39,668  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 64
2016-11-09 14:26:39,668  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_32_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,668  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 63
2016-11-09 14:26:39,669  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_31_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,669  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 62
2016-11-09 14:26:39,669  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 4
2016-11-09 14:26:39,670  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_30_piece0 on localhost:30044 in memory (size: 558.0 B, free: 529.8 MB)
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 61
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 60
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 59
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 58
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 57
2016-11-09 14:26:39,670  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 56
2016-11-09 14:26:39,671  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 55
2016-11-09 14:26:39,671  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,671  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 54
2016-11-09 14:26:39,671  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_29_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,671  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 53
2016-11-09 14:26:39,672  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_28_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,672  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 52
2016-11-09 14:26:39,672  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 3
2016-11-09 14:26:39,673  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_27_piece0 on localhost:30044 in memory (size: 657.0 B, free: 529.8 MB)
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 51
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 50
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 49
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 48
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 47
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 46
2016-11-09 14:26:39,673  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 45
2016-11-09 14:26:39,674  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 44
2016-11-09 14:26:39,674  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 43
2016-11-09 14:26:39,674  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 97
2016-11-09 14:26:39,674  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 9
2016-11-09 14:26:39,674  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_45_piece0 on localhost:30044 in memory (size: 353.0 B, free: 529.8 MB)
2016-11-09 14:26:39,675  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 96
2016-11-09 14:26:39,675  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 95
2016-11-09 14:26:39,675  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 94
2016-11-09 14:26:39,675  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 93
2016-11-09 14:26:39,675  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,675  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_44_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,675  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,675  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 92
2016-11-09 14:26:39,676  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_43_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,676  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 91
2016-11-09 14:26:39,676  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 8
2016-11-09 14:26:39,677  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 90
2016-11-09 14:26:39,677  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 89
2016-11-09 14:26:39,677  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 88
2016-11-09 14:26:39,677  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 87
2016-11-09 14:26:39,677  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_41_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,677  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 86
2016-11-09 14:26:39,678  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_40_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,678  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 85
2016-11-09 14:26:39,678  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 7
2016-11-09 14:26:39,679  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_39_piece0 on localhost:30044 in memory (size: 348.0 B, free: 529.8 MB)
2016-11-09 14:26:39,679  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 84
2016-11-09 14:26:39,679  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 83
2016-11-09 14:26:39,679  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 82
2016-11-09 14:26:39,679  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 81
2016-11-09 14:26:39,679  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_38_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,680  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 80
2016-11-09 14:26:39,680  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,680  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_37_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,680  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 79
2016-11-09 14:26:39,680  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 6
2016-11-09 14:26:39,681  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_36_piece0 on localhost:30044 in memory (size: 462.0 B, free: 529.9 MB)
2016-11-09 14:26:39,681  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 78
2016-11-09 14:26:39,681  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 77
2016-11-09 14:26:39,681  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 76
2016-11-09 14:26:39,681  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_53_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,682  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 110
2016-11-09 14:26:39,682  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_52_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,682  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 109
2016-11-09 14:26:39,682  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 11
2016-11-09 14:26:39,683  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_51_piece0 on localhost:30044 in memory (size: 339.0 B, free: 529.9 MB)
2016-11-09 14:26:39,683  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 108
2016-11-09 14:26:39,683  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 107
2016-11-09 14:26:39,683  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 106
2016-11-09 14:26:39,683  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 105
2016-11-09 14:26:39,684  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_50_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,684  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 104
2016-11-09 14:26:39,684  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_49_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,684  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 103
2016-11-09 14:26:39,685  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 10
2016-11-09 14:26:39,685  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_48_piece0 on localhost:30044 in memory (size: 335.0 B, free: 529.9 MB)
2016-11-09 14:26:39,685  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 102
2016-11-09 14:26:39,685  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 101
2016-11-09 14:26:39,685  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 100
2016-11-09 14:26:39,685  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 99
2016-11-09 14:26:39,686  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_47_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,686  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 98
2016-11-09 14:26:39,686  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_46_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,688  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 36.0 (TID 36). 2654 bytes result sent to driver
2016-11-09 14:26:39,690  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 36.0 (TID 36) in 23 ms on localhost (1/1)
2016-11-09 14:26:39,690  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 36 (mapPartitions at KMeans.scala:279) finished in 0.024 s
2016-11-09 14:26:39,690  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 36.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,690  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,691  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,691  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 37)
2016-11-09 14:26:39,691  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,691  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 37 (ShuffledRDD[57] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,692  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_56 stored as values in memory (estimated size 2.9 KB, free 779.7 KB)
2016-11-09 14:26:39,693  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_56_piece0 stored as bytes in memory (estimated size 1660.0 B, free 781.3 KB)
2016-11-09 14:26:39,694  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_56_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,694  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 56 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,694  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[57] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,694  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 37.0 with 1 tasks
2016-11-09 14:26:39,695  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 37.0 (TID 37, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,695  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 37.0 (TID 37)
2016-11-09 14:26:39,696  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,696  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,701  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 37.0 (TID 37). 1760 bytes result sent to driver
2016-11-09 14:26:39,704  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 37.0 (TID 37) in 9 ms on localhost (1/1)
2016-11-09 14:26:39,704  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 37 (collectAsMap at KMeans.scala:302) finished in 0.009 s
2016-11-09 14:26:39,704  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 37.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,704  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 24 finished: collectAsMap at KMeans.scala:302, took 0.065282 s
2016-11-09 14:26:39,705  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_57 stored as values in memory (estimated size 1496.0 B, free 782.8 KB)
2016-11-09 14:26:39,708  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_57_piece0 stored as bytes in memory (estimated size 327.0 B, free 783.1 KB)
2016-11-09 14:26:39,708  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_57_piece0 in memory on localhost:30044 (size: 327.0 B, free: 529.9 MB)
2016-11-09 14:26:39,708  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 57 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,715  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,716  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 58 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,716  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 25 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,716  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 39 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,716  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 38)
2016-11-09 14:26:39,716  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 38)
2016-11-09 14:26:39,717  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 38 (MapPartitionsRDD[58] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,718  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_58 stored as values in memory (estimated size 15.5 KB, free 798.6 KB)
2016-11-09 14:26:39,721  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.7 KB, free 806.3 KB)
2016-11-09 14:26:39,721  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_58_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,721  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 58 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,721  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[58] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,722  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 38.0 with 1 tasks
2016-11-09 14:26:39,722  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 38.0 (TID 38, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,722  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 38.0 (TID 38)
2016-11-09 14:26:39,725  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,729  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,730  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,734  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,742  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 38.0 (TID 38). 2654 bytes result sent to driver
2016-11-09 14:26:39,745  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 38.0 (TID 38) in 23 ms on localhost (1/1)
2016-11-09 14:26:39,745  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 38 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 38.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 39)
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,746  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 39 (ShuffledRDD[59] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,747  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_59 stored as values in memory (estimated size 2.9 KB, free 809.2 KB)
2016-11-09 14:26:39,749  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_59_piece0 stored as bytes in memory (estimated size 1659.0 B, free 810.8 KB)
2016-11-09 14:26:39,749  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_59_piece0 in memory on localhost:30044 (size: 1659.0 B, free: 529.9 MB)
2016-11-09 14:26:39,749  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 59 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,749  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 39 (ShuffledRDD[59] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,749  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 39.0 with 1 tasks
2016-11-09 14:26:39,750  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 39.0 (TID 39, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,750  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 39.0 (TID 39)
2016-11-09 14:26:39,751  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,751  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,757  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 39.0 (TID 39). 1760 bytes result sent to driver
2016-11-09 14:26:39,760  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 39.0 (TID 39) in 10 ms on localhost (1/1)
2016-11-09 14:26:39,760  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 39 (collectAsMap at KMeans.scala:302) finished in 0.010 s
2016-11-09 14:26:39,760  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 39.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,760  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 25 finished: collectAsMap at KMeans.scala:302, took 0.045146 s
2016-11-09 14:26:39,761  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 5 finished in 13 iterations
2016-11-09 14:26:39,762  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_60 stored as values in memory (estimated size 1136.0 B, free 811.9 KB)
2016-11-09 14:26:39,764  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_60_piece0 stored as bytes in memory (estimated size 291.0 B, free 812.2 KB)
2016-11-09 14:26:39,764  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_60_piece0 in memory on localhost:30044 (size: 291.0 B, free: 529.9 MB)
2016-11-09 14:26:39,765  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 60 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,771  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 60 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 26 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 41 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,772  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 40)
2016-11-09 14:26:39,773  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 40)
2016-11-09 14:26:39,773  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 40 (MapPartitionsRDD[60] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,774  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_61 stored as values in memory (estimated size 15.4 KB, free 827.6 KB)
2016-11-09 14:26:39,776  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_61_piece0 stored as bytes in memory (estimated size 7.7 KB, free 835.3 KB)
2016-11-09 14:26:39,777  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_61_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,777  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 61 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,777  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[60] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,777  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 40.0 with 1 tasks
2016-11-09 14:26:39,778  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 40.0 (TID 40, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,778  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 40.0 (TID 40)
2016-11-09 14:26:39,781  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,786  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,786  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,791  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,798  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 40.0 (TID 40). 2630 bytes result sent to driver
2016-11-09 14:26:39,800  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 40.0 (TID 40) in 22 ms on localhost (1/1)
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 40 (mapPartitions at KMeans.scala:279) finished in 0.022 s
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 40.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 41)
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,801  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 41 (ShuffledRDD[61] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,802  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_62 stored as values in memory (estimated size 2.9 KB, free 838.3 KB)
2016-11-09 14:26:39,804  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_62_piece0 stored as bytes in memory (estimated size 1659.0 B, free 839.9 KB)
2016-11-09 14:26:39,804  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_62_piece0 in memory on localhost:30044 (size: 1659.0 B, free: 529.9 MB)
2016-11-09 14:26:39,804  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 62 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,804  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 41 (ShuffledRDD[61] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,804  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 41.0 with 1 tasks
2016-11-09 14:26:39,805  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 41.0 (TID 41, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,805  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 41.0 (TID 41)
2016-11-09 14:26:39,806  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,806  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,811  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 41.0 (TID 41). 1612 bytes result sent to driver
2016-11-09 14:26:39,813  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 41.0 (TID 41) in 8 ms on localhost (1/1)
2016-11-09 14:26:39,813  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 41 (collectAsMap at KMeans.scala:302) finished in 0.008 s
2016-11-09 14:26:39,813  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 41.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,814  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 26 finished: collectAsMap at KMeans.scala:302, took 0.041932 s
2016-11-09 14:26:39,814  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 7 finished in 14 iterations
2016-11-09 14:26:39,815  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_63 stored as values in memory (estimated size 768.0 B, free 840.6 KB)
2016-11-09 14:26:39,817  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_63_piece0 stored as bytes in memory (estimated size 239.0 B, free 840.9 KB)
2016-11-09 14:26:39,818  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_63_piece0 in memory on localhost:30044 (size: 239.0 B, free: 529.9 MB)
2016-11-09 14:26:39,818  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 63 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,825  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,826  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 62 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,826  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 27 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,826  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 43 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,826  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 42)
2016-11-09 14:26:39,826  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 42)
2016-11-09 14:26:39,827  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 42 (MapPartitionsRDD[62] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,828  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_64 stored as values in memory (estimated size 15.4 KB, free 856.2 KB)
2016-11-09 14:26:39,831  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_64_piece0 stored as bytes in memory (estimated size 7.7 KB, free 864.0 KB)
2016-11-09 14:26:39,832  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_64_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:39,832  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 64 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,832  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[62] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,832  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 42.0 with 1 tasks
2016-11-09 14:26:39,833  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 42.0 (TID 42, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,833  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 42.0 (TID 42)
2016-11-09 14:26:39,837  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,841  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,841  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,845  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,853  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 42.0 (TID 42). 2606 bytes result sent to driver
2016-11-09 14:26:39,855  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 42.0 (TID 42) in 22 ms on localhost (1/1)
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 42 (mapPartitions at KMeans.scala:279) finished in 0.022 s
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 42.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 43)
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,856  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 43 (ShuffledRDD[63] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,857  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_65 stored as values in memory (estimated size 2.9 KB, free 866.9 KB)
2016-11-09 14:26:39,859  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_65_piece0 stored as bytes in memory (estimated size 1660.0 B, free 868.5 KB)
2016-11-09 14:26:39,859  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_65_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:39,859  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 65 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,859  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 43 (ShuffledRDD[63] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,859  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 43.0 with 1 tasks
2016-11-09 14:26:39,860  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 43.0 (TID 43, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,860  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 43.0 (TID 43)
2016-11-09 14:26:39,861  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,862  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 1 ms
2016-11-09 14:26:39,866  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 43.0 (TID 43). 1464 bytes result sent to driver
2016-11-09 14:26:39,868  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 43.0 (TID 43) in 8 ms on localhost (1/1)
2016-11-09 14:26:39,868  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 43 (collectAsMap at KMeans.scala:302) finished in 0.008 s
2016-11-09 14:26:39,868  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 43.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,869  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 27 finished: collectAsMap at KMeans.scala:302, took 0.043682 s
2016-11-09 14:26:39,870  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_66 stored as values in memory (estimated size 768.0 B, free 869.2 KB)
2016-11-09 14:26:39,872  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_66_piece0 stored as bytes in memory (estimated size 225.0 B, free 869.4 KB)
2016-11-09 14:26:39,872  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_66_piece0 in memory on localhost:30044 (size: 225.0 B, free: 529.9 MB)
2016-11-09 14:26:39,872  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 66 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,879  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,880  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 64 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,880  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 28 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,880  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 45 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,880  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 44)
2016-11-09 14:26:39,881  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 44)
2016-11-09 14:26:39,881  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 44 (MapPartitionsRDD[64] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,882  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_67 stored as values in memory (estimated size 15.4 KB, free 884.8 KB)
2016-11-09 14:26:39,885  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_67_piece0 stored as bytes in memory (estimated size 7.7 KB, free 892.5 KB)
2016-11-09 14:26:39,885  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_67_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,885  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 67 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,885  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[64] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,885  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 44.0 with 1 tasks
2016-11-09 14:26:39,886  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 44.0 (TID 44, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,886  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 44.0 (TID 44)
2016-11-09 14:26:39,890  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,894  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,894  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,898  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,905  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 44.0 (TID 44). 2606 bytes result sent to driver
2016-11-09 14:26:39,907  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 44.0 (TID 44) in 21 ms on localhost (1/1)
2016-11-09 14:26:39,907  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 44.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,907  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 44 (mapPartitions at KMeans.scala:279) finished in 0.021 s
2016-11-09 14:26:39,908  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,908  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,908  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 45)
2016-11-09 14:26:39,908  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,908  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 45 (ShuffledRDD[65] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,909  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_68 stored as values in memory (estimated size 2.9 KB, free 895.4 KB)
2016-11-09 14:26:39,911  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_68_piece0 stored as bytes in memory (estimated size 1660.0 B, free 897.1 KB)
2016-11-09 14:26:39,911  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_68_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,911  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 68 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,911  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 45 (ShuffledRDD[65] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,912  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 45.0 with 1 tasks
2016-11-09 14:26:39,912  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 45.0 (TID 45, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,912  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 45.0 (TID 45)
2016-11-09 14:26:39,914  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,914  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:39,918  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 45.0 (TID 45). 1464 bytes result sent to driver
2016-11-09 14:26:39,920  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 45.0 (TID 45) in 8 ms on localhost (1/1)
2016-11-09 14:26:39,920  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 45.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,920  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 45 (collectAsMap at KMeans.scala:302) finished in 0.008 s
2016-11-09 14:26:39,920  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 28 finished: collectAsMap at KMeans.scala:302, took 0.041040 s
2016-11-09 14:26:39,921  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_69 stored as values in memory (estimated size 768.0 B, free 897.8 KB)
2016-11-09 14:26:39,923  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_69_piece0 stored as bytes in memory (estimated size 226.0 B, free 898.0 KB)
2016-11-09 14:26:39,924  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_69_piece0 in memory on localhost:30044 (size: 226.0 B, free: 529.8 MB)
2016-11-09 14:26:39,924  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 69 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,931  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,931  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 66 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,932  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 29 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,932  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 47 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,932  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 46)
2016-11-09 14:26:39,932  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 46)
2016-11-09 14:26:39,932  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 46 (MapPartitionsRDD[66] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,933  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_70 stored as values in memory (estimated size 15.4 KB, free 913.4 KB)
2016-11-09 14:26:39,935  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_70_piece0 stored as bytes in memory (estimated size 7.7 KB, free 921.1 KB)
2016-11-09 14:26:39,936  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_70_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,936  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 70 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,936  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[66] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,936  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 46.0 with 1 tasks
2016-11-09 14:26:39,937  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 46.0 (TID 46, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,937  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 46.0 (TID 46)
2016-11-09 14:26:39,940  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,945  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,945  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:39,951  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:39,958  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 46.0 (TID 46). 2606 bytes result sent to driver
2016-11-09 14:26:39,959  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 46.0 (TID 46) in 22 ms on localhost (1/1)
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 46 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 46.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 47)
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:39,960  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 47 (ShuffledRDD[67] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:39,961  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_71 stored as values in memory (estimated size 2.9 KB, free 924.0 KB)
2016-11-09 14:26:39,963  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_71_piece0 stored as bytes in memory (estimated size 1660.0 B, free 925.7 KB)
2016-11-09 14:26:39,963  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_71_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:39,963  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 71 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,963  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 47 (ShuffledRDD[67] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:39,963  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 47.0 with 1 tasks
2016-11-09 14:26:39,964  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 47.0 (TID 47, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:39,965  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 47.0 (TID 47)
2016-11-09 14:26:39,965  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:39,966  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 1 ms
2016-11-09 14:26:39,970  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 47.0 (TID 47). 1464 bytes result sent to driver
2016-11-09 14:26:39,972  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 47.0 (TID 47) in 8 ms on localhost (1/1)
2016-11-09 14:26:39,973  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 47 (collectAsMap at KMeans.scala:302) finished in 0.008 s
2016-11-09 14:26:39,973  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 47.0, whose tasks have all completed, from pool 
2016-11-09 14:26:39,973  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 29 finished: collectAsMap at KMeans.scala:302, took 0.041855 s
2016-11-09 14:26:39,974  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_72 stored as values in memory (estimated size 768.0 B, free 926.4 KB)
2016-11-09 14:26:39,976  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_72_piece0 stored as bytes in memory (estimated size 226.0 B, free 926.6 KB)
2016-11-09 14:26:39,976  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_72_piece0 in memory on localhost:30044 (size: 226.0 B, free: 529.8 MB)
2016-11-09 14:26:39,976  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 72 from broadcast at KMeans.scala:276
2016-11-09 14:26:39,983  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collectAsMap at KMeans.scala:302
2016-11-09 14:26:39,984  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Registering RDD 68 (mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,984  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 30 (collectAsMap at KMeans.scala:302) with 1 output partitions
2016-11-09 14:26:39,984  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 49 (collectAsMap at KMeans.scala:302)
2016-11-09 14:26:39,984  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List(ShuffleMapStage 48)
2016-11-09 14:26:39,984  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List(ShuffleMapStage 48)
2016-11-09 14:26:39,985  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ShuffleMapStage 48 (MapPartitionsRDD[68] at mapPartitions at KMeans.scala:279), which has no missing parents
2016-11-09 14:26:39,986  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_73 stored as values in memory (estimated size 15.4 KB, free 942.0 KB)
2016-11-09 14:26:39,988  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_73_piece0 stored as bytes in memory (estimated size 7.7 KB, free 949.7 KB)
2016-11-09 14:26:39,988  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_73_piece0 in memory on localhost:30044 (size: 7.7 KB, free: 529.8 MB)
2016-11-09 14:26:39,989  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 73 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:39,989  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[68] at mapPartitions at KMeans.scala:279)
2016-11-09 14:26:39,989  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 48.0 with 1 tasks
2016-11-09 14:26:39,990  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 48.0 (TID 48, localhost, partition 0,ANY, 2951 bytes)
2016-11-09 14:26:39,990  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 48.0 (TID 48)
2016-11-09 14:26:39,993  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:39,997  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:39,997  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:40,001  INFO  org.apache.spark.storage.BlockManager - logInfo: Found block rdd_7_0 locally
2016-11-09 14:26:40,009  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 48.0 (TID 48). 2606 bytes result sent to driver
2016-11-09 14:26:40,011  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 48.0 (TID 48) in 22 ms on localhost (1/1)
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 48.0, whose tasks have all completed, from pool 
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ShuffleMapStage 48 (mapPartitions at KMeans.scala:279) finished in 0.023 s
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: looking for newly runnable stages
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: running: Set()
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: waiting: Set(ResultStage 49)
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: failed: Set()
2016-11-09 14:26:40,012  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 49 (ShuffledRDD[69] at reduceByKey at KMeans.scala:302), which has no missing parents
2016-11-09 14:26:40,013  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_74 stored as values in memory (estimated size 2.9 KB, free 952.6 KB)
2016-11-09 14:26:40,015  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_74_piece0 stored as bytes in memory (estimated size 1660.0 B, free 954.3 KB)
2016-11-09 14:26:40,015  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_74_piece0 in memory on localhost:30044 (size: 1660.0 B, free: 529.8 MB)
2016-11-09 14:26:40,016  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 74 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:40,016  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 49 (ShuffledRDD[69] at reduceByKey at KMeans.scala:302)
2016-11-09 14:26:40,016  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 49.0 with 1 tasks
2016-11-09 14:26:40,017  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 49.0 (TID 49, localhost, partition 0,NODE_LOCAL, 2483 bytes)
2016-11-09 14:26:40,017  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 49.0 (TID 49)
2016-11-09 14:26:40,018  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Getting 1 non-empty blocks out of 1 blocks
2016-11-09 14:26:40,018  INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - logInfo: Started 0 remote fetches in 0 ms
2016-11-09 14:26:40,022  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 49.0 (TID 49). 1464 bytes result sent to driver
2016-11-09 14:26:40,025  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 49.0 (TID 49) in 9 ms on localhost (1/1)
2016-11-09 14:26:40,025  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 49 (collectAsMap at KMeans.scala:302) finished in 0.009 s
2016-11-09 14:26:40,025  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 49.0, whose tasks have all completed, from pool 
2016-11-09 14:26:40,025  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 30 finished: collectAsMap at KMeans.scala:302, took 0.042248 s
2016-11-09 14:26:40,026  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 0 finished in 18 iterations
2016-11-09 14:26:40,026  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Run 3 finished in 18 iterations
2016-11-09 14:26:40,027  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: Iterations took 1.105 seconds.
2016-11-09 14:26:40,027  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: KMeans converged in 18 iterations.
2016-11-09 14:26:40,028  INFO  org.apache.spark.mllib.clustering.KMeans - logInfo: The cost for the best run is 1.1866909560005711E7.
2016-11-09 14:26:40,029  INFO  org.apache.spark.rdd.MapPartitionsRDD - logInfo: Removing RDD 7 from persistence list
2016-11-09 14:26:40,030  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 7
2016-11-09 14:26:40,030  WARN  org.apache.spark.mllib.clustering.KMeans - logWarning: The input data was not directly cached, which may hurt performance if its parent RDDs are also uncached.
2016-11-09 14:26:40,038  INFO  org.apache.spark.SparkContext - logInfo: Starting job: collect at Kmeans_new.java:52
2016-11-09 14:26:40,039  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 31 (collect at Kmeans_new.java:52) with 1 output partitions
2016-11-09 14:26:40,039  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 50 (collect at Kmeans_new.java:52)
2016-11-09 14:26:40,039  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:40,039  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:40,039  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 50 (MapPartitionsRDD[70] at map at Kmeans_new.java:51), which has no missing parents
2016-11-09 14:26:40,040  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_75 stored as values in memory (estimated size 13.9 KB, free 894.0 KB)
2016-11-09 14:26:40,043  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_75_piece0 stored as bytes in memory (estimated size 7.1 KB, free 901.1 KB)
2016-11-09 14:26:40,043  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_75_piece0 in memory on localhost:30044 (size: 7.1 KB, free: 529.9 MB)
2016-11-09 14:26:40,043  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 75 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:40,043  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[70] at map at Kmeans_new.java:51)
2016-11-09 14:26:40,044  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 50.0 with 1 tasks
2016-11-09 14:26:40,044  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 50.0 (TID 50, localhost, partition 0,ANY, 2788 bytes)
2016-11-09 14:26:40,044  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 50.0 (TID 50)
2016-11-09 14:26:40,048  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:40,052  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:40,052  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:40,077  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 50.0 (TID 50). 41733 bytes result sent to driver
2016-11-09 14:26:40,080  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 50.0 (TID 50) in 36 ms on localhost (1/1)
2016-11-09 14:26:40,080  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 50 (collect at Kmeans_new.java:52) finished in 0.036 s
2016-11-09 14:26:40,080  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 50.0, whose tasks have all completed, from pool 
2016-11-09 14:26:40,080  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 31 finished: collect at Kmeans_new.java:52, took 0.041796 s
[172.0] --->0
[42.0] --->0
[45.0] --->0
[19.0] --->0
[24.0] --->0
[47.0] --->0
[517.0] --->4
[1012.0] --->3
[621.0] --->4
[157.0] --->0
[65.0] --->0
[58.0] --->0
[37.0] --->0
[969.0] --->3
[111.0] --->0
[31.0] --->0
[4.0] --->0
[56.0] --->0
[25.0] --->0
[884.0] --->3
[48.0] --->0
[1050.0] --->3
[3.0] --->0
[38.0] --->0
[23.0] --->0
[24.0] --->0
[150.0] --->0
[3878.0] --->2
[200.0] --->0
[44.0] --->0
[1779.0] --->1
[27.0] --->0
[390.0] --->4
[52.0] --->0
[35.0] --->0
[31.0] --->0
[24.0] --->0
[59.0] --->0
[16.0] --->0
[41.0] --->0
[36.0] --->0
[35.0] --->0
[25.0] --->0
[46.0] --->0
[52.0] --->0
[44.0] --->0
[201.0] --->0
[43.0] --->0
[44.0] --->0
[21.0] --->0
[26.0] --->0
[50.0] --->0
[10.0] --->0
[2870.0] --->2
[44.0] --->0
[8.0] --->0
[50.0] --->0
[28.0] --->0
[39.0] --->0
[45.0] --->0
[32.0] --->0
[1213.0] --->3
[174.0] --->0
[43.0] --->0
[48.0] --->0
[19.0] --->0
[27.0] --->0
[49.0] --->0
[159.0] --->0
[31.0] --->0
[86.0] --->0
[79.0] --->0
[67.0] --->0
[68.0] --->0
[23.0] --->0
[60.0] --->0
[64.0] --->0
[53.0] --->0
[68.0] --->0
[28.0] --->0
[51.0] --->0
[67.0] --->0
[45.0] --->0
[78.0] --->0
[27.0] --->0
[53.0] --->0
[67.0] --->0
[69.0] --->0
[117.0] --->0
[52.0] --->0
[60.0] --->0
[47.0] --->0
[989.0] --->3
[774.0] --->3
[166.0] --->0
[103.0] --->0
[35.0] --->0
[18.0] --->0
[33.0] --->0
[25.0] --->0
[30.0] --->0
[47.0] --->0
[21.0] --->0
[34.0] --->0
[33.0] --->0
[35.0] --->0
[40.0] --->0
[18.0] --->0
[34.0] --->0
[31.0] --->0
[33.0] --->0
[35.0] --->0
[17.0] --->0
[32.0] --->0
[30.0] --->0
[32.0] --->0
[35.0] --->0
[18.0] --->0
[30.0] --->0
[26.0] --->0
[28.0] --->0
[39.0] --->0
[20.0] --->0
[30.0] --->0
[26.0] --->0
[29.0] --->0
[39.0] --->0
[21.0] --->0
[35.0] --->0
[31.0] --->0
[36.0] --->0
[44.0] --->0
[22.0] --->0
[36.0] --->0
[35.0] --->0
[40.0] --->0
[35.0] --->0
[18.0] --->0
[31.0] --->0
[27.0] --->0
[30.0] --->0
[21.0] --->0
[27.0] --->0
[31.0] --->0
[101.0] --->0
[34.0] --->0
[17.0] --->0
[18.0] --->0
[43.0] --->0
[20.0] --->0
[32.0] --->0
[34.0] --->0
[104.0] --->0
[40.0] --->0
[26.0] --->0
[59.0] --->0
[15.0] --->0
[25.0] --->0
[28.0] --->0
[82.0] --->0
[35.0] --->0
[20.0] --->0
[45.0] --->0
[16.0] --->0
[31.0] --->0
[39.0] --->0
[38.0] --->0
[18.0] --->0
[43.0] --->0
[17.0] --->0
[29.0] --->0
[34.0] --->0
[40.0] --->0
[22.0] --->0
[50.0] --->0
[18.0] --->0
[37.0] --->0
[42.0] --->0
[43.0] --->0
[23.0] --->0
[50.0] --->0
[22.0] --->0
[47.0] --->0
[49.0] --->0
[51.0] --->0
[27.0] --->0
[65.0] --->0
[50.0] --->0
[52.0] --->0
[40.0] --->0
[131.0] --->0
[52.0] --->0
[38.0] --->0
[70.0] --->0
[202.0] --->0
[2663.0] --->2
[28.0] --->0
[37.0] --->0
[24.0] --->0
[31.0] --->0
[52.0] --->0
[125.0] --->0
[17.0] --->0
[45.0] --->0
[48.0] --->0
[23.0] --->0
[29.0] --->0
[51.0] --->0
[50.0] --->0
[354.0] --->4
[24.0] --->0
[73.0] --->0
[17.0] --->0
[23.0] --->0
[44.0] --->0
[23.0] --->0
[190.0] --->0
[477.0] --->4
[62.0] --->0
[47.0] --->0
[12.0] --->0
[40.0] --->0
[102.0] --->0
[312.0] --->4
[447.0] --->4
[47.0] --->0
[35.0] --->0
[38.0] --->0
[45.0] --->0
[10.0] --->0
[13.0] --->0
[38.0] --->0
[35.0] --->0
[51.0] --->0
[14.0] --->0
[38.0] --->0
[37.0] --->0
[19.0] --->0
[47.0] --->0
[53.0] --->0
[22.0] --->0
[32.0] --->0
[59.0] --->0
[3.0] --->0
[21.0] --->0
[271.0] --->4
[8.0] --->0
[33.0] --->0
[67.0] --->0
[24.0] --->0
[44.0] --->0
[46.0] --->0
[1585.0] --->1
[194.0] --->0
[6.0] --->0
[10.0] --->0
[51.0] --->0
[32.0] --->0
[50.0] --->0
[48.0] --->0
[31.0] --->0
[25.0] --->0
[40.0] --->0
[61.0] --->0
[28.0] --->0
[50.0] --->0
[26.0] --->0
[30.0] --->0
[604.0] --->4
[674.0] --->4
[387.0] --->4
[54.0] --->0
[13.0] --->0
[18.0] --->0
[47.0] --->0
[13.0] --->0
[86.0] --->0
[16.0] --->0
[79.0] --->0
[172.0] --->0
[5.0] --->0
[13.0] --->0
[11.0] --->0
[31.0] --->0
[13.0] --->0
[32.0] --->0
[37.0] --->0
[5.0] --->0
[20.0] --->0
[14.0] --->0
[45.0] --->0
[11.0] --->0
[35.0] --->0
[18.0] --->0
[37.0] --->0
[11.0] --->0
[18.0] --->0
[12.0] --->0
[43.0] --->0
[17.0] --->0
[39.0] --->0
[36.0] --->0
[15.0] --->0
[39.0] --->0
[40.0] --->0
[22.0] --->0
[24.0] --->0
[63.0] --->0
[47.0] --->0
[48.0] --->0
[29.0] --->0
[37.0] --->0
[58.0] --->0
[26.0] --->0
[49.0] --->0
[32.0] --->0
[26.0] --->0
[50.0] --->0
[24.0] --->0
[47.0] --->0
[19.0] --->0
[25.0] --->0
[45.0] --->0
[75.0] --->0
[21.0] --->0
[42.0] --->0
[45.0] --->0
[29.0] --->0
[30.0] --->0
[62.0] --->0
[24.0] --->0
[41.0] --->0
[40.0] --->0
[45.0] --->0
[39.0] --->0
[51.0] --->0
[36.0] --->0
[32.0] --->0
[25.0] --->0
[49.0] --->0
[143.0] --->0
[65.0] --->0
[44.0] --->0
[39.0] --->0
[32.0] --->0
[45.0] --->0
[15.0] --->0
[12.0] --->0
[30.0] --->0
[44.0] --->0
[18.0] --->0
[21.0] --->0
[2554.0] --->2
[38.0] --->0
[14.0] --->0
[12.0] --->0
[25.0] --->0
[23.0] --->0
[42.0] --->0
[33.0] --->0
[19.0] --->0
[11.0] --->0
[25.0] --->0
[23.0] --->0
[20.0] --->0
[27.0] --->0
[10.0] --->0
[21.0] --->0
[11.0] --->0
[27.0] --->0
[25.0] --->0
[52.0] --->0
[14.0] --->0
[11.0] --->0
[26.0] --->0
[23.0] --->0
[17.0] --->0
[19.0] --->0
[9.0] --->0
[26.0] --->0
[23.0] --->0
[37.0] --->0
[18.0] --->0
[8.0] --->0
[24.0] --->0
[22.0] --->0
[15.0] --->0
[19.0] --->0
[9.0] --->0
[30.0] --->0
[23.0] --->0
[16.0] --->0
[130.0] --->0
[72.0] --->0
[40.0] --->0
[104.0] --->0
[66.0] --->0
[36.0] --->0
[31.0] --->0
[165.0] --->0
[64.0] --->0
[27.0] --->0
[35.0] --->0
[47.0] --->0
[36.0] --->0
[29.0] --->0
[28.0] --->0
[83.0] --->0
[28.0] --->0
[42.0] --->0
[54.0] --->0
[43.0] --->0
[29.0] --->0
[37.0] --->0
[46.0] --->0
[54.0] --->0
[20.0] --->0
[32.0] --->0
[60.0] --->0
[6.0] --->0
[20.0] --->0
[27.0] --->0
[82.0] --->0
[30.0] --->0
[45.0] --->0
[65.0] --->0
[83.0] --->0
[32.0] --->0
[35.0] --->0
[79.0] --->0
[26.0] --->0
[45.0] --->0
[46.0] --->0
[47.0] --->0
[134.0] --->0
[39.0] --->0
[62.0] --->0
[24.0] --->0
[37.0] --->0
[42.0] --->0
[37.0] --->0
[29.0] --->0
[34.0] --->0
[125.0] --->0
[27.0] --->0
[44.0] --->0
[80.0] --->0
[53.0] --->0
[34.0] --->0
[37.0] --->0
[52.0] --->0
[47.0] --->0
[30.0] --->0
[38.0] --->0
[56.0] --->0
[31.0] --->0
[49.0] --->0
[48.0] --->0
[31.0] --->0
[64.0] --->0
[65.0] --->0
[76.0] --->0
[28.0] --->0
[19.0] --->0
[25.0] --->0
[52.0] --->0
[90.0] --->0
[64.0] --->0
[43.0] --->0
[23.0] --->0
[40.0] --->0
[49.0] --->0
[30.0] --->0
[27.0] --->0
[46.0] --->0
[46.0] --->0
[43.0] --->0
[28.0] --->0
[38.0] --->0
[45.0] --->0
[49.0] --->0
[55.0] --->0
[23.0] --->0
[28.0] --->0
[55.0] --->0
[54.0] --->0
[35.0] --->0
[21.0] --->0
[32.0] --->0
[40.0] --->0
[67.0] --->0
[45.0] --->0
[49.0] --->0
[32.0] --->0
[26.0] --->0
[45.0] --->0
[50.0] --->0
[25.0] --->0
[2596.0] --->2
[12.0] --->0
[18.0] --->0
[23.0] --->0
[51.0] --->0
[42.0] --->0
[14.0] --->0
[24.0] --->0
[40.0] --->0
[19.0] --->0
[34.0] --->0
[50.0] --->0
[38.0] --->0
[17.0] --->0
[38.0] --->0
[1578.0] --->1
[40.0] --->0
[18.0] --->0
[39.0] --->0
[29.0] --->0
[49.0] --->0
[24.0] --->0
[20.0] --->0
[31.0] --->0
[38.0] --->0
[14.0] --->0
[36.0] --->0
[30.0] --->0
[49.0] --->0
[25.0] --->0
[22.0] --->0
[31.0] --->0
[41.0] --->0
[18.0] --->0
[37.0] --->0
[30.0] --->0
[49.0] --->0
[25.0] --->0
[22.0] --->0
[31.0] --->0
[52.0] --->0
[18.0] --->0
[40.0] --->0
[34.0] --->0
[50.0] --->0
[22.0] --->0
[19.0] --->0
[30.0] --->0
[50.0] --->0
[18.0] --->0
[42.0] --->0
[32.0] --->0
[53.0] --->0
[25.0] --->0
[17.0] --->0
[34.0] --->0
[16.0] --->0
[43.0] --->0
[48.0] --->0
[23.0] --->0
[29.0] --->0
[47.0] --->0
[17.0] --->0
[28.0] --->0
[49.0] --->0
[24.0] --->0
[29.0] --->0
[55.0] --->0
[17.0] --->0
[27.0] --->0
[16.0] --->0
[22.0] --->0
[5.0] --->0
[7.0] --->0
[51.0] --->0
[10.0] --->0
[20.0] --->0
[35.0] --->0
[16.0] --->0
[21.0] --->0
[36.0] --->0
[87.0] --->0
[30.0] --->0
[11.0] --->0
[17.0] --->0
[49.0] --->0
[55.0] --->0
[35.0] --->0
[65.0] --->0
[40.0] --->0
[32.0] --->0
[45.0] --->0
[19.0] --->0
[34.0] --->0
[43.0] --->0
[36.0] --->0
[14.0] --->0
[46.0] --->0
[23.0] --->0
[30.0] --->0
[49.0] --->0
[35.0] --->0
[14.0] --->0
[39.0] --->0
[22.0] --->0
[31.0] --->0
[16.0] --->0
[41.0] --->0
[46.0] --->0
[31.0] --->0
[28.0] --->0
[49.0] --->0
[47.0] --->0
[53.0] --->0
[34.0] --->0
[30.0] --->0
[51.0] --->0
[57.0] --->0
[51.0] --->0
[36.0] --->0
[44.0] --->0
[34.0] --->0
[31.0] --->0
[9.0] --->0
[26.0] --->0
[46.0] --->0
[22.0] --->0
[28.0] --->0
[49.0] --->0
[65.0] --->0
[18.0] --->0
[28.0] --->0
[16.0] --->0
[104.0] --->0
[43.0] --->0
[16.0] --->0
[20.0] --->0
[26.0] --->0
[42.0] --->0
[21.0] --->0
[28.0] --->0
[15.0] --->0
[29.0] --->0
[23.0] --->0
[17.0] --->0
[21.0] --->0
[26.0] --->0
[36.0] --->0
[19.0] --->0
[21.0] --->0
[13.0] --->0
[25.0] --->0
[39.0] --->0
[15.0] --->0
[66.0] --->0
[24.0] --->0
[30.0] --->0
[18.0] --->0
[24.0] --->0
[15.0] --->0
[24.0] --->0
[40.0] --->0
[15.0] --->0
[57.0] --->0
[13.0] --->0
[51.0] --->0
[48.0] --->0
[32.0] --->0
[46.0] --->0
[53.0] --->0
[35.0] --->0
[360.0] --->4
[781.0] --->3
[52.0] --->0
[51.0] --->0
[33.0] --->0
[41.0] --->0
[71.0] --->0
[19.0] --->0
[27.0] --->0
[49.0] --->0
[20.0] --->0
[30.0] --->0
[54.0] --->0
[1094.0] --->3
[50.0] --->0
[93.0] --->0
[31.0] --->0
[63.0] --->0
[2.0] --->0
[178.0] --->0
[997.0] --->3
[55.0] --->0
[129.0] --->0
[807.0] --->3
[135.0] --->0
[187.0] --->0
[23.0] --->0
[18.0] --->0
[141.0] --->0
[103.0] --->0
[193.0] --->0
[47.0] --->0
[26.0] --->0
[158.0] --->0
[90.0] --->0
[478.0] --->4
[850.0] --->3
[124.0] --->0
[22.0] --->0
[17.0] --->0
[53.0] --->0
[71.0] --->0
[35.0] --->0
[27.0] --->0
[134.0] --->0
[21.0] --->0
[19.0] --->0
[58.0] --->0
[79.0] --->0
[35.0] --->0
[28.0] --->0
[155.0] --->0
[7.0] --->0
[38.0] --->0
[20.0] --->0
[93.0] --->0
[69.0] --->0
[43.0] --->0
[32.0] --->0
[57.0] --->0
[54.0] --->0
[34.0] --->0
[48.0] --->0
[53.0] --->0
[35.0] --->0
[57.0] --->0
[66.0] --->0
[40.0] --->0
[51.0] --->0
[52.0] --->0
[7.0] --->0
[18.0] --->0
[48.0] --->0
[50.0] --->0
[23.0] --->0
[27.0] --->0
[50.0] --->0
[124.0] --->0
[1167.0] --->3
[454.0] --->4
[129.0] --->0
[25.0] --->0
[184.0] --->0
[4.0] --->0
[23.0] --->0
[43.0] --->0
[21.0] --->0
[23.0] --->0
[41.0] --->0
[98.0] --->0
[101.0] --->0
[1645.0] --->1
[106.0] --->0
[30.0] --->0
[54.0] --->0
[39.0] --->0
[41.0] --->0
[66.0] --->0
[28.0] --->0
[68.0] --->0
[20.0] --->0
[45.0] --->0
[34.0] --->0
[37.0] --->0
[48.0] --->0
[32.0] --->0
[54.0] --->0
[22.0] --->0
[43.0] --->0
[30.0] --->0
[48.0] --->0
[35.0] --->0
[21.0] --->0
[60.0] --->0
[34.0] --->0
[31.0] --->0
[45.0] --->0
[35.0] --->0
[24.0] --->0
[72.0] --->0
[49.0] --->0
[48.0] --->0
[46.0] --->0
[29.0] --->0
[51.0] --->0
[47.0] --->0
[34.0] --->0
[44.0] --->0
[47.0] --->0
[32.0] --->0
[26.0] --->0
[47.0] --->0
[1013.0] --->3
[125.0] --->0
[42.0] --->0
[29.0] --->0
[1768.0] --->1
[22.0] --->0
[55.0] --->0
[31.0] --->0
[24.0] --->0
[73.0] --->0
[803.0] --->3
[1563.0] --->1
[517.0] --->4
[52.0] --->0
[52.0] --->0
[33.0] --->0
[43.0] --->0
[53.0] --->0
[1534.0] --->1
[421.0] --->4
[378.0] --->4
[123.0] --->0
[96.0] --->0
[127.0] --->0
[97.0] --->0
[22.0] --->0
[56.0] --->0
[333.0] --->4
[185.0] --->0
[97.0] --->0
[42.0] --->0
[67.0] --->0
[68.0] --->0
[26.0] --->0
[1727.0] --->1
[3721.0] --->2
[47.0] --->0
[102.0] --->0
[53.0] --->0
[151.0] --->0
[75.0] --->0
[59.0] --->0
[135.0] --->0
[38.0] --->0
[119.0] --->0
[82.0] --->0
[134.0] --->0
[101.0] --->0
[3.0] --->0
[54.0] --->0
[9.0] --->0
[20.0] --->0
[48.0] --->0
[33.0] --->0
[12.0] --->0
[74.0] --->0
[77.0] --->0
[3.0] --->0
[11.0] --->0
[12.0] --->0
[32.0] --->0
[36.0] --->0
[32.0] --->0
[47.0] --->0
[59.0] --->0
[5.0] --->0
[18.0] --->0
[15.0] --->0
[32.0] --->0
[59.0] --->0
[24.0] --->0
[35.0] --->0
[76.0] --->0
[4.0] --->0
[27.0] --->0
[21.0] --->0
[49.0] --->0
[41.0] --->0
[17.0] --->0
[10.0] --->0
[44.0] --->0
[42.0] --->0
[8.0] --->0
[19.0] --->0
[13.0] --->0
[29.0] --->0
[40.0] --->0
[17.0] --->0
[25.0] --->0
[36.0] --->0
[3.0] --->0
[22.0] --->0
[10.0] --->0
[20.0] --->0
[36.0] --->0
[18.0] --->0
[35.0] --->0
[62.0] --->0
[4.0] --->0
[11.0] --->0
[10.0] --->0
[25.0] --->0
[35.0] --->0
[18.0] --->0
[43.0] --->0
[41.0] --->0
[3.0] --->0
[11.0] --->0
[10.0] --->0
[21.0] --->0
[12.0] --->0
[14.0] --->0
[20.0] --->0
[68.0] --->0
[3.0] --->0
[26.0] --->0
[11.0] --->0
[62.0] --->0
[14.0] --->0
[34.0] --->0
[5.0] --->0
[11.0] --->0
[39.0] --->0
[3.0] --->0
[9.0] --->0
[9.0] --->0
[20.0] --->0
[10.0] --->0
[38.0] --->0
[15.0] --->0
[39.0] --->0
[4.0] --->0
[12.0] --->0
[13.0] --->0
[22.0] --->0
[34.0] --->0
[17.0] --->0
[22.0] --->0
[53.0] --->0
[3.0] --->0
[11.0] --->0
[10.0] --->0
[21.0] --->0
[38.0] --->0
[19.0] --->0
[12.0] --->0
[22.0] --->0
[373.0] --->4
[4.0] --->0
[14.0] --->0
[11.0] --->0
[31.0] --->0
[37.0] --->0
[23.0] --->0
[142.0] --->0
[58.0] --->0
[3.0] --->0
[10.0] --->0
[8.0] --->0
[17.0] --->0
[31.0] --->0
[15.0] --->0
[23.0] --->0
[32.0] --->0
[5.0] --->0
[10.0] --->0
[9.0] --->0
[15.0] --->0
[31.0] --->0
[16.0] --->0
[23.0] --->0
[41.0] --->0
[3.0] --->0
[12.0] --->0
[9.0] --->0
[21.0] --->0
[33.0] --->0
[16.0] --->0
[32.0] --->0
[17.0] --->0
[37.0] --->0
[45.0] --->0
[29.0] --->0
[27.0] --->0
[46.0] --->0
[98.0] --->0
[52.0] --->0
[67.0] --->0
[31.0] --->0
[87.0] --->0
[363.0] --->4
[86.0] --->0
[52.0] --->0
[56.0] --->0
[43.0] --->0
[16.0] --->0
[34.0] --->0
[27.0] --->0
[7.0] --->0
[2684.0] --->2
[147.0] --->0
[46.0] --->0
[34.0] --->0
[58.0] --->0
[58.0] --->0
[45.0] --->0
[58.0] --->0
[27.0] --->0
[47.0] --->0
[1273.0] --->3
[40.0] --->0
[41.0] --->0
[29.0] --->0
[34.0] --->0
[347.0] --->4
[12.0] --->0
[36.0] --->0
[33.0] --->0
[30.0] --->0
[32.0] --->0
[17.0] --->0
[35.0] --->0
[17.0] --->0
[28.0] --->0
[24.0] --->0
[30.0] --->0
[34.0] --->0
[19.0] --->0
[29.0] --->0
[25.0] --->0
[31.0] --->0
[37.0] --->0
[17.0] --->0
[32.0] --->0
[27.0] --->0
[31.0] --->0
[49.0] --->0
[23.0] --->0
[31.0] --->0
[54.0] --->0
[65.0] --->0
[25.0] --->0
[322.0] --->4
[415.0] --->4
[736.0] --->3
[24.0] --->0
[559.0] --->4
[54.0] --->0
[47.0] --->0
[56.0] --->0
[31.0] --->0
[37.0] --->0
[141.0] --->0
[420.0] --->4
[347.0] --->4
[33.0] --->0
[46.0] --->0
[19.0] --->0
[27.0] --->0
[45.0] --->0
[45.0] --->0
[56.0] --->0
[24.0] --->0
[46.0] --->0
[246.0] --->4
[623.0] --->4
[23.0] --->0
[33.0] --->0
[42.0] --->0
[101.0] --->0
[47.0] --->0
[43.0] --->0
[23.0] --->0
[53.0] --->0
[41.0] --->0
[20.0] --->0
[48.0] --->0
[73.0] --->0
[90.0] --->0
[41.0] --->0
[22.0] --->0
[57.0] --->0
[30.0] --->0
[61.0] --->0
[22.0] --->0
[71.0] --->0
[59.0] --->0
[55.0] --->0
[21.0] --->0
[43.0] --->0
[95.0] --->0
[52.0] --->0
[26.0] --->0
[59.0] --->0
[33.0] --->0
[8.0] --->0
[33.0] --->0
[17.0] --->0
[34.0] --->0
[45.0] --->0
[54.0] --->0
[31.0] --->0
[271.0] --->4
[83.0] --->0
[23.0] --->0
[108.0] --->0
[115.0] --->0
[30.0] --->0
[43.0] --->0
[51.0] --->0
[26.0] --->0
[46.0] --->0
[16.0] --->0
[46.0] --->0
[48.0] --->0
[29.0] --->0
[26.0] --->0
[48.0] --->0
[594.0] --->4
[292.0] --->4
[23.0] --->0
[726.0] --->3
[65.0] --->0
[42.0] --->0
[34.0] --->0
[53.0] --->0
[23.0] --->0
[44.0] --->0
[165.0] --->0
[87.0] --->0
[38.0] --->0
[658.0] --->4
[656.0] --->4
[29.0] --->0
[25.0] --->0
[29.0] --->0
[47.0] --->0
[38.0] --->0
[40.0] --->0
[28.0] --->0
[29.0] --->0
[27.0] --->0
[40.0] --->0
[11.0] --->0
[15.0] --->0
[27.0] --->0
[24.0] --->0
[20.0] --->0
[11.0] --->0
[32.0] --->0
[28.0] --->0
[27.0] --->0
[44.0] --->0
[17.0] --->0
[9.0] --->0
[64.0] --->0
[29.0] --->0
[25.0] --->0
[49.0] --->0
[61.0] --->0
[205.0] --->0
[99.0] --->0
[130.0] --->0
[539.0] --->4
[116.0] --->0
[162.0] --->0
[215.0] --->0
[4.0] --->0
[9.0] --->0
[2.0] --->0
[33.0] --->0
[46.0] --->0
[23.0] --->0
[25.0] --->0
[55.0] --->0
[51.0] --->0
[37.0] --->0
[41.0] --->0
[58.0] --->0
[18.0] --->0
[22.0] --->0
[15.0] --->0
[5.0] --->0
[12.0] --->0
[26.0] --->0
[52.0] --->0
[33.0] --->0
[44.0] --->0
[963.0] --->3
[22.0] --->0
[89.0] --->0
[21.0] --->0
[47.0] --->0
[63.0] --->0
[115.0] --->0
[1521.0] --->1
[47.0] --->0
[12.0] --->0
[16.0] --->0
[13.0] --->0
[41.0] --->0
[49.0] --->0
[7.0] --->0
[30.0] --->0
[751.0] --->3
[53.0] --->0
[35.0] --->0
[7.0] --->0
[18.0] --->0
[13.0] --->0
[35.0] --->0
[35.0] --->0
[21.0] --->0
[45.0] --->0
[18.0] --->0
[14.0] --->0
[55.0] --->0
[50.0] --->0
[23.0] --->0
[17.0] --->0
[39.0] --->0
[45.0] --->0
[27.0] --->0
[24.0] --->0
[43.0] --->0
[182.0] --->0
[231.0] --->0
[160.0] --->0
[45.0] --->0
[43.0] --->0
[30.0] --->0
[37.0] --->0
[45.0] --->0
[29.0] --->0
[522.0] --->4
[481.0] --->4
[44.0] --->0
[43.0] --->0
[27.0] --->0
[14.0] --->0
[8.0] --->0
[32.0] --->0
[63.0] --->0
[378.0] --->4
[28.0] --->0
[49.0] --->0
[20.0] --->0
[26.0] --->0
[48.0] --->0
[818.0] --->3
[374.0] --->4
[10.0] --->0
[58.0] --->0
[404.0] --->4
[182.0] --->0
[103.0] --->0
[6.0] --->0
[198.0] --->0
[17.0] --->0
[412.0] --->4
[450.0] --->4
[32.0] --->0
[49.0] --->0
[16.0] --->0
[26.0] --->0
[4.0] --->0
[29.0] --->0
[26.0] --->0
[66.0] --->0
[39.0] --->0
[2077.0] --->1
[9.0] --->0
[17.0] --->0
[46.0] --->0
[30.0] --->0
[26.0] --->0
[260.0] --->4
[100.0] --->0
[19.0] --->0
[43.0] --->0
[33.0] --->0
[34.0] --->0
[45.0] --->0
[88.0] --->0
[50.0] --->0
[37.0] --->0
[29.0] --->0
[41.0] --->0
[52.0] --->0
[31.0] --->0
[11.0] --->0
[36.0] --->0
[61.0] --->0
[57.0] --->0
[25.0] --->0
[40.0] --->0
[14.0] --->0
[28.0] --->0
[47.0] --->0
[43.0] --->0
[34.0] --->0
[24.0] --->0
[56.0] --->0
[16.0] --->0
[23.0] --->0
[31.0] --->0
[18.0] --->0
[177.0] --->0
[23.0] --->0
[59.0] --->0
[50.0] --->0
[26.0] --->0
[55.0] --->0
[43.0] --->0
[43.0] --->0
[26.0] --->0
[51.0] --->0
[29.0] --->0
[36.0] --->0
[54.0] --->0
[41.0] --->0
[48.0] --->0
[1567.0] --->1
[16.0] --->0
[20.0] --->0
[75.0] --->0
[415.0] --->4
[30.0] --->0
[58.0] --->0
[29.0] --->0
[155.0] --->0
[36.0] --->0
[20.0] --->0
[44.0] --->0
[56.0] --->0
[30.0] --->0
[29.0] --->0
[120.0] --->0
[25.0] --->0
[39.0] --->0
[86.0] --->0
[59.0] --->0
[66.0] --->0
[43.0] --->0
[44.0] --->0
[74.0] --->0
[24.0] --->0
[36.0] --->0
[64.0] --->0
[42.0] --->0
[26.0] --->0
[31.0] --->0
[79.0] --->0
[22.0] --->0
[35.0] --->0
[82.0] --->0
[49.0] --->0
[27.0] --->0
[32.0] --->0
[78.0] --->0
[24.0] --->0
[37.0] --->0
[46.0] --->0
[43.0] --->0
[35.0] --->0
[34.0] --->0
[108.0] --->0
[26.0] --->0
[43.0] --->0
[58.0] --->0
[59.0] --->0
[38.0] --->0
[38.0] --->0
[148.0] --->0
[27.0] --->0
[41.0] --->0
[39.0] --->0
[55.0] --->0
[30.0] --->0
[40.0] --->0
[15.0] --->0
[83.0] --->0
[43.0] --->0
[28.0] --->0
[24.0] --->0
[44.0] --->0
[42.0] --->0
[52.0] --->0
[32.0] --->0
[26.0] --->0
[54.0] --->0
[17.0] --->0
[41.0] --->0
[46.0] --->0
[30.0] --->0
[24.0] --->0
[46.0] --->0
[45.0] --->0
[45.0] --->0
[31.0] --->0
[25.0] --->0
[44.0] --->0
[45.0] --->0
[50.0] --->0
[33.0] --->0
[26.0] --->0
[49.0] --->0
[25.0] --->0
[49.0] --->0
[27.0] --->0
[52.0] --->0
[33.0] --->0
[24.0] --->0
[45.0] --->0
[35.0] --->0
[28.0] --->0
[46.0] --->0
[92.0] --->0
[26.0] --->0
[70.0] --->0
[104.0] --->0
[46.0] --->0
[33.0] --->0
[26.0] --->0
[106.0] --->0
[22.0] --->0
[72.0] --->0
[169.0] --->0
[45.0] --->0
[95.0] --->0
[25.0] --->0
[79.0] --->0
[105.0] --->0
[62.0] --->0
[34.0] --->0
[78.0] --->0
[21.0] --->0
[55.0] --->0
[71.0] --->0
[51.0] --->0
[77.0] --->0
[22.0] --->0
[48.0] --->0
[64.0] --->0
[54.0] --->0
[86.0] --->0
[24.0] --->0
[57.0] --->0
[85.0] --->0
[91.0] --->0
[105.0] --->0
[24.0] --->0
[30.0] --->0
[47.0] --->0
[77.0] --->0
[55.0] --->0
[78.0] --->0
[22.0] --->0
[50.0] --->0
[64.0] --->0
[47.0] --->0
[220.0] --->0
[30.0] --->0
[18.0] --->0
[49.0] --->0
[92.0] --->0
[114.0] --->0
[26.0] --->0
[29.0] --->0
[155.0] --->0
[180.0] --->0
[27.0] --->0
[17.0] --->0
[71.0] --->0
[118.0] --->0
[65.0] --->0
[32.0] --->0
[141.0] --->0
[27.0] --->0
[17.0] --->0
[55.0] --->0
[83.0] --->0
[26.0] --->0
[64.0] --->0
[32.0] --->0
[127.0] --->0
[23.0] --->0
[17.0] --->0
[45.0] --->0
[51.0] --->0
[44.0] --->0
[31.0] --->0
[131.0] --->0
[31.0] --->0
[19.0] --->0
[59.0] --->0
[67.0] --->0
[39.0] --->0
[29.0] --->0
[121.0] --->0
[34.0] --->0
[18.0] --->0
[48.0] --->0
[62.0] --->0
[37.0] --->0
[30.0] --->0
[103.0] --->0
[31.0] --->0
[20.0] --->0
[47.0] --->0
[61.0] --->0
[41.0] --->0
[37.0] --->0
[102.0] --->0
[31.0] --->0
[44.0] --->0
[49.0] --->0
[56.0] --->0
[35.0] --->0
[31.0] --->0
[44.0] --->0
[42.0] --->0
[22.0] --->0
[36.0] --->0
[33.0] --->0
[32.0] --->0
[17.0] --->0
[17.0] --->0
[687.0] --->4
[21.0] --->0
[20.0] --->0
[17.0] --->0
[3.0] --->0
[40.0] --->0
[25.0] --->0
[48.0] --->0
[35.0] --->0
[20.0] --->0
[41.0] --->0
[228.0] --->0
[44.0] --->0
[1888.0] --->1
[1473.0] --->1
[31.0] --->0
[2593.0] --->2
[42.0] --->0
[26.0] --->0
[10.0] --->0
[14.0] --->0
[10.0] --->0
[23.0] --->0
[103.0] --->0
[1136.0] --->3
[3242.0] --->2
[70.0] --->0
[76.0] --->0
[34.0] --->0
[77.0] --->0
[2456.0] --->2
[69.0] --->0
[56.0] --->0
[54.0] --->0
[34.0] --->0
[50.0] --->0
[85.0] --->0
[38.0] --->0
[39.0] --->0
[54.0] --->0
[19.0] --->0
[17.0] --->0
[35.0] --->0
[174.0] --->0
[40.0] --->0
[30.0] --->0
[13.0] --->0
[35.0] --->0
[16.0] --->0
[29.0] --->0
[40.0] --->0
[30.0] --->0
[12.0] --->0
[38.0] --->0
[15.0] --->0
[29.0] --->0
[9.0] --->0
[42.0] --->0
[33.0] --->0
[13.0] --->0
[28.0] --->0
[15.0] --->0
[29.0] --->0
[51.0] --->0
[30.0] --->0
[13.0] --->0
[28.0] --->0
[14.0] --->0
[30.0] --->0
[46.0] --->0
[28.0] --->0
[13.0] --->0
[32.0] --->0
[16.0] --->0
[32.0] --->0
[48.0] --->0
[28.0] --->0
[13.0] --->0
[32.0] --->0
[15.0] --->0
[36.0] --->0
[48.0] --->0
[28.0] --->0
[14.0] --->0
[33.0] --->0
[16.0] --->0
[30.0] --->0
[15.0] --->0
[39.0] --->0
[47.0] --->0
[28.0] --->0
[24.0] --->0
[43.0] --->0
[526.0] --->4
[14.0] --->0
[26.0] --->0
[39.0] --->0
[2.0] --->0
[24.0] --->0
[41.0] --->0
[26.0] --->0
[16.0] --->0
[28.0] --->0
[49.0] --->0
[28.0] --->0
[26.0] --->0
[45.0] --->0
[741.0] --->3
[651.0] --->4
[549.0] --->4
[17.0] --->0
[42.0] --->0
[28.0] --->0
[22.0] --->0
[7.0] --->0
[10.0] --->0
[616.0] --->4
[38.0] --->0
[12.0] --->0
[111.0] --->0
[105.0] --->0
[35.0] --->0
[743.0] --->3
[43.0] --->0
[155.0] --->0
[22.0] --->0
[42.0] --->0
[36.0] --->0
[21.0] --->0
[28.0] --->0
[6.0] --->0
[8.0] --->0
[19.0] --->0
[35.0] --->0
[152.0] --->0
[26.0] --->0
[438.0] --->4
[28.0] --->0
[51.0] --->0
[39.0] --->0
[18.0] --->0
[37.0] --->0
[43.0] --->0
[50.0] --->0
[24.0] --->0
[17.0] --->0
[36.0] --->0
[51.0] --->0
[21.0] --->0
[43.0] --->0
[35.0] --->0
[53.0] --->0
[27.0] --->0
[24.0] --->0
[43.0] --->0
[53.0] --->0
[22.0] --->0
[44.0] --->0
[40.0] --->0
[52.0] --->0
[28.0] --->0
[24.0] --->0
[35.0] --->0
[16.0] --->0
[38.0] --->0
[46.0] --->0
[19.0] --->0
[26.0] --->0
[42.0] --->0
[45.0] --->0
[50.0] --->0
[19.0] --->0
[27.0] --->0
[49.0] --->0
[3614.0] --->2
[113.0] --->0
[365.0] --->4
[65.0] --->0
[18.0] --->0
[504.0] --->4
[16.0] --->0
[29.0] --->0
[50.0] --->0
[30.0] --->0
[37.0] --->0
[8.0] --->0
[34.0] --->0
[16.0] --->0
[11.0] --->0
[642.0] --->4
[47.0] --->0
[34.0] --->0
[26.0] --->0
[47.0] --->0
[2596.0] --->2
[41.0] --->0
[111.0] --->0
[5.0] --->0
[629.0] --->4
[372.0] --->4
[96.0] --->0
[18.0] --->0
[29.0] --->0
[604.0] --->4
[51.0] --->0
[1474.0] --->1
[39.0] --->0
[54.0] --->0
[43.0] --->0
[182.0] --->0
[50.0] --->0
[91.0] --->0
[91.0] --->0
[462.0] --->4
[220.0] --->0
[30.0] --->0
[93.0] --->0
[4.0] --->0
[77.0] --->0
[121.0] --->0
[42.0] --->0
[34.0] --->0
[36.0] --->0
[113.0] --->0
[46.0] --->0
[4.0] --->0
[75.0] --->0
[64.0] --->0
[39.0] --->0
[29.0] --->0
[22.0] --->0
[96.0] --->0
[16.0] --->0
[40.0] --->0
[40.0] --->0
[32.0] --->0
[27.0] --->0
[45.0] --->0
[35.0] --->0
[47.0] --->0
[47.0] --->0
[30.0] --->0
[39.0] --->0
[29.0] --->0
[270.0] --->4
[125.0] --->0
[19.0] --->0
[47.0] --->0
[34.0] --->0
[23.0] --->0
[44.0] --->0
[61.0] --->0
[10.0] --->0
[46.0] --->0
[31.0] --->0
[24.0] --->0
[44.0] --->0
[250.0] --->4
[231.0] --->0
[717.0] --->3
[259.0] --->4
[24.0] --->0
[7.0] --->0
[3416.0] --->2
[881.0] --->3
[16.0] --->0
[26.0] --->0
[36.0] --->0
[93.0] --->0
[35.0] --->0
[25.0] --->0
[19.0] --->0
[45.0] --->0
[108.0] --->0
[24.0] --->0
[54.0] --->0
[40.0] --->0
[144.0] --->0
[54.0] --->0
[23.0] --->0
[24.0] --->0
[57.0] --->0
[20.0] --->0
[44.0] --->0
[56.0] --->0
[54.0] --->0
[29.0] --->0
[63.0] --->0
[22.0] --->0
[43.0] --->0
[50.0] --->0
[46.0] --->0
[23.0] --->0
[24.0] --->0
[55.0] --->0
[926.0] --->3
[786.0] --->3
[17.0] --->0
[43.0] --->0
[48.0] --->0
[4.0] --->0
[79.0] --->0
[23.0] --->0
[45.0] --->0
[1192.0] --->3
[410.0] --->4
[234.0] --->0
[840.0] --->3
[50.0] --->0
[66.0] --->0
[15.0] --->0
[33.0] --->0
[4.0] --->0
[49.0] --->0
[440.0] --->4
[364.0] --->4
[17.0] --->0
[25.0] --->0
[45.0] --->0
[17.0] --->0
[16.0] --->0
[17.0] --->0
[14.0] --->0
[16.0] --->0
[9.0] --->0
[38.0] --->0
[22.0] --->0
[44.0] --->0
[20.0] --->0
[23.0] --->0
[42.0] --->0
[294.0] --->4
[210.0] --->0
[23.0] --->0
[159.0] --->0
[58.0] --->0
[18.0] --->0
[25.0] --->0
[8.0] --->0
[37.0] --->0
[42.0] --->0
[56.0] --->0
[45.0] --->0
[26.0] --->0
[692.0] --->3
[231.0] --->0
[130.0] --->0
[35.0] --->0
[482.0] --->4
[52.0] --->0
[18.0] --->0
[29.0] --->0
[16.0] --->0
[47.0] --->0
[52.0] --->0
[18.0] --->0
[30.0] --->0
[34.0] --->0
[1011.0] --->3
[45.0] --->0
[20.0] --->0
[28.0] --->0
[15.0] --->0
[40.0] --->0
[25.0] --->0
[49.0] --->0
[18.0] --->0
[40.0] --->0
[56.0] --->0
[20.0] --->0
[29.0] --->0
[15.0] --->0
[42.0] --->0
[98.0] --->0
[17.0] --->0
[32.0] --->0
[30.0] --->0
[39.0] --->0
[63.0] --->0
[31.0] --->0
[25.0] --->0
[47.0] --->0
[85.0] --->0
[103.0] --->0
[389.0] --->4
[20.0] --->0
[51.0] --->0
[35.0] --->0
[27.0] --->0
[9.0] --->0
[27.0] --->0
[139.0] --->0
[29.0] --->0
[34.0] --->0
[67.0] --->0
[55.0] --->0
[36.0] --->0
[1070.0] --->3
[32.0] --->0
[41.0] --->0
[45.0] --->0
[32.0] --->0
[24.0] --->0
[44.0] --->0
[156.0] --->0
[49.0] --->0
[143.0] --->0
[1143.0] --->3
[420.0] --->4
[338.0] --->4
[87.0] --->0
[402.0] --->4
[1276.0] --->3
[7.0] --->0
[16.0] --->0
[144.0] --->0
[247.0] --->4
[260.0] --->4
[894.0] --->3
[6.0] --->0
[19.0] --->0
[12.0] --->0
[39.0] --->0
[54.0] --->0
[50.0] --->0
[755.0] --->3
[1524.0] --->1
[33.0] --->0
[17.0] --->0
[31.0] --->0
[26.0] --->0
[31.0] --->0
[37.0] --->0
[18.0] --->0
[33.0] --->0
[28.0] --->0
[30.0] --->0
[17.0] --->0
[43.0] --->0
[49.0] --->0
[35.0] --->0
[28.0] --->0
[54.0] --->0
[47.0] --->0
[48.0] --->0
[35.0] --->0
[25.0] --->0
[50.0] --->0
[17.0] --->0
[43.0] --->0
[46.0] --->0
[32.0] --->0
[25.0] --->0
[48.0] --->0
[40.0] --->0
[24.0] --->0
[46.0] --->0
[33.0] --->0
[543.0] --->4
[28.0] --->0
[54.0] --->0
[37.0] --->0
[27.0] --->0
[52.0] --->0
[994.0] --->3
[21.0] --->0
[762.0] --->3
[48.0] --->0
[165.0] --->0
[87.0] --->0
[639.0] --->4
[1657.0] --->1
[598.0] --->4
[320.0] --->4
[17.0] --->0
[33.0] --->0
[927.0] --->3
[47.0] --->0
[30.0] --->0
[813.0] --->3
[633.0] --->4
[19.0] --->0
[77.0] --->0
[49.0] --->0
[93.0] --->0
[2376.0] --->2
[45.0] --->0
[21.0] --->0
[48.0] --->0
[45.0] --->0
[48.0] --->0
[27.0] --->0
[23.0] --->0
[37.0] --->0
[49.0] --->0
[21.0] --->0
[42.0] --->0
[34.0] --->0
[52.0] --->0
[29.0] --->0
[22.0] --->0
[35.0] --->0
[18.0] --->0
[41.0] --->0
[48.0] --->0
[31.0] --->0
[26.0] --->0
[46.0] --->0
[170.0] --->0
[52.0] --->0
[56.0] --->0
[108.0] --->0
[94.0] --->0
[327.0] --->4
[53.0] --->0
[38.0] --->0
[55.0] --->0
[106.0] --->0
[24.0] --->0
[42.0] --->0
[19.0] --->0
[24.0] --->0
[45.0] --->0
[46.0] --->0
[8.0] --->0
[2.0] --->0
[44.0] --->0
[30.0] --->0
[37.0] --->0
[809.0] --->3
[126.0] --->0
[27.0] --->0
[45.0] --->0
[29.0] --->0
[23.0] --->0
[46.0] --->0
[88.0] --->0
[70.0] --->0
[35.0] --->0
[47.0] --->0
[57.0] --->0
[50.0] --->0
[41.0] --->0
[57.0] --->0
[39.0] --->0
[36.0] --->0
[47.0] --->0
[42.0] --->0
[37.0] --->0
[54.0] --->0
[19.0] --->0
[12.0] --->0
[41.0] --->0
[54.0] --->0
[41.0] --->0
[36.0] --->0
[278.0] --->4
[39.0] --->0
[15.0] --->0
[18.0] --->0
[8.0] --->0
[27.0] --->0
[34.0] --->0
[11.0] --->0
[39.0] --->0
[11.0] --->0
[13.0] --->0
[26.0] --->0
[31.0] --->0
[15.0] --->0
[32.0] --->0
[734.0] --->3
[41.0] --->0
[65.0] --->0
[29.0] --->0
[12.0] --->0
[31.0] --->0
[22.0] --->0
[478.0] --->4
[1020.0] --->3
[996.0] --->3
[583.0] --->4
[676.0] --->4
[6.0] --->0
[1466.0] --->1
[41.0] --->0
[21.0] --->0
[15.0] --->0
[40.0] --->0
[39.0] --->0
[14.0] --->0
[22.0] --->0
[39.0] --->0
[10.0] --->0
[14.0] --->0
[25.0] --->0
[12.0] --->0
[41.0] --->0
[20.0] --->0
[43.0] --->0
[14.0] --->0
[14.0] --->0
[93.0] --->0
[46.0] --->0
[21.0] --->0
[62.0] --->0
[27.0] --->0
[15.0] --->0
[43.0] --->0
[14.0] --->0
[18.0] --->0
[54.0] --->0
[60.0] --->0
[26.0] --->0
[13.0] --->0
[33.0] --->0
[15.0] --->0
[40.0] --->0
[41.0] --->0
[24.0] --->0
[13.0] --->0
[27.0] --->0
[18.0] --->0
[10.0] --->0
[29.0] --->0
[45.0] --->0
[27.0] --->0
[14.0] --->0
[32.0] --->0
[15.0] --->0
[31.0] --->0
[56.0] --->0
[32.0] --->0
[15.0] --->0
[31.0] --->0
[17.0] --->0
[33.0] --->0
[49.0] --->0
[26.0] --->0
[13.0] --->0
[29.0] --->0
[15.0] --->0
[32.0] --->0
[49.0] --->0
[46.0] --->0
[14.0] --->0
[31.0] --->0
[15.0] --->0
[34.0] --->0
[47.0] --->0
[49.0] --->0
[14.0] --->0
[32.0] --->0
[18.0] --->0
[38.0] --->0
[18.0] --->0
[42.0] --->0
[47.0] --->0
[51.0] --->0
[32.0] --->0
[25.0] --->0
[13.0] --->0
[48.0] --->0
[43.0] --->0
[48.0] --->0
[31.0] --->0
[19.0] --->0
[266.0] --->4
[203.0] --->0
[869.0] --->3
[48.0] --->0
[16.0] --->0
[24.0] --->0
[124.0] --->0
[56.0] --->0
[23.0] --->0
[55.0] --->0
[158.0] --->0
[92.0] --->0
[7.0] --->0
[1553.0] --->1
[921.0] --->3
[550.0] --->4
[52.0] --->0
[1.0] --->0
[78.0] --->0
[220.0] --->0
[99.0] --->0
[497.0] --->4
[1234.0] --->3
[51.0] --->0
[14.0] --->0
[28.0] --->0
[574.0] --->4
[49.0] --->0
[39.0] --->0
[27.0] --->0
[29.0] --->0
[24.0] --->0
[36.0] --->0
[5.0] --->0
[27.0] --->0
[26.0] --->0
[22.0] --->0
[37.0] --->0
[6.0] --->0
[34.0] --->0
[30.0] --->0
[26.0] --->0
[48.0] --->0
[46.0] --->0
[46.0] --->0
[31.0] --->0
[41.0] --->0
[30.0] --->0
[35.0] --->0
[614.0] --->4
[666.0] --->4
[288.0] --->4
[313.0] --->4
[70.0] --->0
[645.0] --->4
[1846.0] --->1
[8.0] --->0
[46.0] --->0
[54.0] --->0
[23.0] --->0
[29.0] --->0
[52.0] --->0
[17.0] --->0
[46.0] --->0
[12.0] --->0
[136.0] --->0
[15.0] --->0
[90.0] --->0
[11.0] --->0
[23.0] --->0
[308.0] --->4
[174.0] --->0
[192.0] --->0
[224.0] --->0
[93.0] --->0
[110.0] --->0
[29.0] --->0
[999.0] --->3
[20.0] --->0
[43.0] --->0
[18.0] --->0
[40.0] --->0
[541.0] --->4
[355.0] --->4
[82.0] --->0
[23.0] --->0
[34.0] --->0
[43.0] --->0
[43.0] --->0
[26.0] --->0
[27.0] --->0
[220.0] --->0
[28.0] --->0
[39.0] --->0
[44.0] --->0
[39.0] --->0
[185.0] --->0
[33.0] --->0
[81.0] --->0
[25.0] --->0
[41.0] --->0
[86.0] --->0
[7.0] --->0
[35.0] --->0
[38.0] --->0
[886.0] --->3
[38.0] --->0
[44.0] --->0
[84.0] --->0
[302.0] --->4
[38.0] --->0
[35.0] --->0
[20.0] --->0
[47.0] --->0
[87.0] --->0
[36.0] --->0
[32.0] --->0
[56.0] --->0
[46.0] --->0
[58.0] --->0
[35.0] --->0
[30.0] --->0
[49.0] --->0
[48.0] --->0
[45.0] --->0
[30.0] --->0
[34.0] --->0
[28.0] --->0
[31.0] --->0
[22.0] --->0
[40.0] --->0
[27.0] --->0
[23.0] --->0
[41.0] --->0
[42.0] --->0
[51.0] --->0
[34.0] --->0
[28.0] --->0
[49.0] --->0
[53.0] --->0
[54.0] --->0
[34.0] --->0
[41.0] --->0
[74.0] --->0
[39.0] --->0
[45.0] --->0
[31.0] --->0
[27.0] --->0
[50.0] --->0
[111.0] --->0
[20.0] --->0
[31.0] --->0
[1115.0] --->3
[32.0] --->0
[13.0] --->0
[6.0] --->0
[17.0] --->0
[36.0] --->0
[25.0] --->0
[31.0] --->0
[35.0] --->0
[15.0] --->0
[28.0] --->0
[25.0] --->0
[29.0] --->0
[14.0] --->0
[7.0] --->0
[17.0] --->0
[32.0] --->0
[26.0] --->0
[26.0] --->0
[78.0] --->0
[55.0] --->0
[124.0] --->0
[50.0] --->0
[2697.0] --->2
[160.0] --->0
[21.0] --->0
[54.0] --->0
[77.0] --->0
[147.0] --->0
[228.0] --->0
[1196.0] --->3
[107.0] --->0
[43.0] --->0
[103.0] --->0
[74.0] --->0
[58.0] --->0
[153.0] --->0
[84.0] --->0
[228.0] --->0
[892.0] --->3
[135.0] --->0
[33.0] --->0
[6.0] --->0
[95.0] --->0
[28.0] --->0
[47.0] --->0
[77.0] --->0
[48.0] --->0
[116.0] --->0
[28.0] --->0
[64.0] --->0
[67.0] --->0
[70.0] --->0
[247.0] --->4
[17.0] --->0
[42.0] --->0
[48.0] --->0
[31.0] --->0
[28.0] --->0
[87.0] --->0
[55.0] --->0
[51.0] --->0
[33.0] --->0
[45.0] --->0
[57.0] --->0
[834.0] --->3
[1035.0] --->3
[160.0] --->0
[18.0] --->0
[42.0] --->0
[49.0] --->0
[22.0] --->0
[26.0] --->0
[50.0] --->0
[122.0] --->0
[24.0] --->0
[47.0] --->0
[21.0] --->0
[26.0] --->0
[48.0] --->0
[133.0] --->0
[13.0] --->0
[1458.0] --->1
[1880.0] --->1
[32.0] --->0
[35.0] --->0
[35.0] --->0
[180.0] --->0
[60.0] --->0
[66.0] --->0
[111.0] --->0
[19.0] --->0
[351.0] --->4
[21.0] --->0
[59.0] --->0
[22.0] --->0
[70.0] --->0
[12.0] --->0
[91.0] --->0
[98.0] --->0
[30.0] --->0
[93.0] --->0
[2459.0] --->2
[27.0] --->0
[34.0] --->0
[5.0] --->0
[48.0] --->0
[133.0] --->0
[27.0] --->0
[182.0] --->0
[50.0] --->0
[28.0] --->0
[63.0] --->0
[7.0] --->0
[109.0] --->0
[115.0] --->0
[37.0] --->0
[23.0] --->0
[62.0] --->0
[125.0] --->0
[2424.0] --->2
[42.0] --->0
[16.0] --->0
[21.0] --->0
[13.0] --->0
[24.0] --->0
[46.0] --->0
[37.0] --->0
[30.0] --->0
[27.0] --->0
[2075.0] --->1
[1026.0] --->3
[273.0] --->4
[341.0] --->4
[127.0] --->0
[473.0] --->4
[185.0] --->0
[55.0] --->0
[21.0] --->0
[29.0] --->0
[15.0] --->0
[33.0] --->0
[49.0] --->0
[24.0] --->0
[32.0] --->0
[29.0] --->0
[46.0] --->0
[19.0] --->0
[23.0] --->0
[15.0] --->0
[31.0] --->0
[35.0] --->0
[14.0] --->0
[29.0] --->0
[25.0] --->0
[47.0] --->0
[17.0] --->0
[20.0] --->0
[10.0] --->0
[20.0] --->0
[41.0] --->0
[23.0] --->0
[29.0] --->0
[25.0] --->0
[27.0] --->0
[18.0] --->0
[22.0] --->0
[13.0] --->0
[22.0] --->0
[42.0] --->0
[14.0] --->0
[30.0] --->0
[21.0] --->0
[51.0] --->0
[17.0] --->0
[22.0] --->0
[13.0] --->0
[22.0] --->0
[46.0] --->0
[14.0] --->0
[29.0] --->0
[24.0] --->0
[42.0] --->0
[19.0] --->0
[28.0] --->0
[17.0] --->0
[29.0] --->0
[51.0] --->0
[17.0] --->0
[34.0] --->0
[29.0] --->0
[43.0] --->0
[17.0] --->0
[21.0] --->0
[13.0] --->0
[23.0] --->0
[39.0] --->0
[14.0] --->0
[30.0] --->0
[25.0] --->0
[28.0] --->0
[21.0] --->0
[27.0] --->0
[15.0] --->0
[29.0] --->0
[35.0] --->0
[17.0] --->0
[35.0] --->0
[26.0] --->0
[58.0] --->0
[24.0] --->0
[26.0] --->0
[16.0] --->0
[27.0] --->0
[50.0] --->0
[16.0] --->0
[32.0] --->0
[24.0] --->0
[1893.0] --->1
[1226.0] --->3
[396.0] --->4
[64.0] --->0
[188.0] --->0
[70.0] --->0
[132.0] --->0
[128.0] --->0
[24.0] --->0
[86.0] --->0
[44.0] --->0
[46.0] --->0
[51.0] --->0
[93.0] --->0
[24.0] --->0
[63.0] --->0
[44.0] --->0
[46.0] --->0
[41.0] --->0
[35.0] --->0
[74.0] --->0
[39.0] --->0
[45.0] --->0
[42.0] --->0
[79.0] --->0
[97.0] --->0
[21.0] --->0
[49.0] --->0
[36.0] --->0
[37.0] --->0
[67.0] --->0
[100.0] --->0
[29.0] --->0
[60.0] --->0
[46.0] --->0
[37.0] --->0
[50.0] --->0
[118.0] --->0
[27.0] --->0
[75.0] --->0
[57.0] --->0
[59.0] --->0
[53.0] --->0
[96.0] --->0
[26.0] --->0
[54.0] --->0
[43.0] --->0
[40.0] --->0
[41.0] --->0
[49.0] --->0
[48.0] --->0
[29.0] --->0
[39.0] --->0
[30.0] --->0
[32.0] --->0
[65.0] --->0
[44.0] --->0
[54.0] --->0
[36.0] --->0
[27.0] --->0
[47.0] --->0
[3184.0] --->2
[15.0] --->0
[47.0] --->0
[52.0] --->0
[34.0] --->0
[30.0] --->0
[45.0] --->0
[6.0] --->0
[1590.0] --->1
[36.0] --->0
[41.0] --->0
[46.0] --->0
[32.0] --->0
[27.0] --->0
[45.0] --->0
[146.0] --->0
[5.0] --->0
[15.0] --->0
[14.0] --->0
[31.0] --->0
[63.0] --->0
[47.0] --->0
[44.0] --->0
[3684.0] --->2
[37.0] --->0
[4.0] --->0
[10.0] --->0
[11.0] --->0
[28.0] --->0
[35.0] --->0
[17.0] --->0
[26.0] --->0
[41.0] --->0
[4.0] --->0
[9.0] --->0
[10.0] --->0
[24.0] --->0
[32.0] --->0
[17.0] --->0
[27.0] --->0
[23.0] --->0
[4.0] --->0
[9.0] --->0
[9.0] --->0
[21.0] --->0
[11.0] --->0
[30.0] --->0
[27.0] --->0
[18.0] --->0
[42.0] --->0
[51.0] --->0
[34.0] --->0
[32.0] --->0
[53.0] --->0
[1969.0] --->1
[288.0] --->4
[24.0] --->0
[11.0] --->0
[40.0] --->0
[34.0] --->0
[28.0] --->0
[53.0] --->0
[17.0] --->0
[27.0] --->0
[52.0] --->0
[31.0] --->0
[28.0] --->0
[48.0] --->0
[26.0] --->0
[50.0] --->0
[37.0] --->0
[30.0] --->0
[51.0] --->0
[179.0] --->0
[246.0] --->4
[47.0] --->0
[31.0] --->0
[27.0] --->0
[22.0] --->0
[42.0] --->0
[41.0] --->0
[30.0] --->0
[30.0] --->0
[23.0] --->0
[42.0] --->0
[47.0] --->0
[28.0] --->0
[31.0] --->0
[23.0] --->0
[39.0] --->0
[28.0] --->0
[29.0] --->0
[15.0] --->0
[41.0] --->0
[44.0] --->0
[18.0] --->0
[101.0] --->0
[33.0] --->0
[29.0] --->0
[53.0] --->0
[206.0] --->0
[188.0] --->0
[224.0] --->0
[955.0] --->3
[200.0] --->0
[23.0] --->0
[47.0] --->0
[30.0] --->0
[26.0] --->0
[46.0] --->0
[48.0] --->0
[48.0] --->0
[35.0] --->0
[39.0] --->0
[31.0] --->0
[36.0] --->0
[776.0] --->3
[40.0] --->0
[90.0] --->0
[10.0] --->0
[131.0] --->0
[10.0] --->0
[44.0] --->0
[48.0] --->0
[21.0] --->0
[24.0] --->0
[45.0] --->0
[110.0] --->0
[6.0] --->0
[37.0] --->0
[3.0] --->0
[10.0] --->0
[8.0] --->0
[513.0] --->4
[31.0] --->0
[15.0] --->0
[32.0] --->0
[82.0] --->0
[223.0] --->0
[40.0] --->0
[4.0] --->0
[8.0] --->0
[9.0] --->0
[25.0] --->0
[34.0] --->0
[15.0] --->0
[24.0] --->0
2016-11-09 14:26:40,100  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_76 stored as values in memory (estimated size 408.0 B, free 901.5 KB)
2016-11-09 14:26:40,102  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_76_piece0 stored as bytes in memory (estimated size 246.0 B, free 901.7 KB)
2016-11-09 14:26:40,103  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_76_piece0 in memory on localhost:30044 (size: 246.0 B, free: 529.9 MB)
2016-11-09 14:26:40,103  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 76 from broadcast at KMeansModel.scala:87
2016-11-09 14:26:40,109  INFO  org.apache.spark.SparkContext - logInfo: Starting job: sum at KMeansModel.scala:88
2016-11-09 14:26:40,109  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 32 (sum at KMeansModel.scala:88) with 1 output partitions
2016-11-09 14:26:40,109  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 51 (sum at KMeansModel.scala:88)
2016-11-09 14:26:40,109  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:40,109  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:40,110  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 51 (MapPartitionsRDD[71] at map at KMeansModel.scala:88), which has no missing parents
2016-11-09 14:26:40,111  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_77 stored as values in memory (estimated size 13.4 KB, free 915.1 KB)
2016-11-09 14:26:40,113  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_77_piece0 stored as bytes in memory (estimated size 6.8 KB, free 922.0 KB)
2016-11-09 14:26:40,113  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_77_piece0 in memory on localhost:30044 (size: 6.8 KB, free: 529.9 MB)
2016-11-09 14:26:40,113  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 77 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:40,113  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[71] at map at KMeansModel.scala:88)
2016-11-09 14:26:40,113  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 51.0 with 1 tasks
2016-11-09 14:26:40,114  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 51.0 (TID 51, localhost, partition 0,ANY, 2788 bytes)
2016-11-09 14:26:40,114  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 51.0 (TID 51)
2016-11-09 14:26:40,117  INFO  org.apache.spark.rdd.HadoopRDD - logInfo: Input split: hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0:0+173289
2016-11-09 14:26:40,121  INFO  org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger - <init>: min key = null, max key = null
2016-11-09 14:26:40,121  INFO  org.apache.hadoop.hive.ql.io.orc.ReaderImpl - rowsOptions: Reading ORC rows from hdfs://nameservice1/user/hive/warehouse/jiangyin.db/entrance_log_time/000000_0 with {include: [true, true, false, false, false, true], offset: 0, length: 9223372036854775807}
2016-11-09 14:26:40,131  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 51.0 (TID 51). 2347 bytes result sent to driver
2016-11-09 14:26:40,133  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 51.0 (TID 51) in 18 ms on localhost (1/1)
2016-11-09 14:26:40,133  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 51.0, whose tasks have all completed, from pool 
2016-11-09 14:26:40,133  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 51 (sum at KMeansModel.scala:88) finished in 0.019 s
2016-11-09 14:26:40,133  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 32 finished: sum at KMeansModel.scala:88, took 0.024303 s
Within Set Sum of Squared Errors = 1.1866909560005711E7
Cluster centers:
 [43.26469443363176]
 [1683.6000000000001]
 [2931.9473684210525]
 [940.542372881356]
 [438.3137254901961]
2016-11-09 14:26:40,150  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,188  INFO  org.apache.spark.SparkContext - logInfo: Starting job: saveAsTextFile at KMeansModel.scala:131
2016-11-09 14:26:40,188  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 33 (saveAsTextFile at KMeansModel.scala:131) with 1 output partitions
2016-11-09 14:26:40,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 52 (saveAsTextFile at KMeansModel.scala:131)
2016-11-09 14:26:40,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:40,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:40,189  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 52 (MapPartitionsRDD[73] at saveAsTextFile at KMeansModel.scala:131), which has no missing parents
2016-11-09 14:26:40,225  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_78 stored as values in memory (estimated size 74.6 KB, free 996.5 KB)
2016-11-09 14:26:40,228  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_78_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1022.9 KB)
2016-11-09 14:26:40,228  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_78_piece0 in memory on localhost:30044 (size: 26.4 KB, free: 529.9 MB)
2016-11-09 14:26:40,229  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 78 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:40,229  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[73] at saveAsTextFile at KMeansModel.scala:131)
2016-11-09 14:26:40,229  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 52.0 with 1 tasks
2016-11-09 14:26:40,233  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 52.0 (TID 52, localhost, partition 0,PROCESS_LOCAL, 2665 bytes)
2016-11-09 14:26:40,233  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 52.0 (TID 52)
2016-11-09 14:26:40,257  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,342  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0052_m_000000_52' to hdfs://nameservice1/usr/machine_learning/model/k_means/metadata/_temporary/0/task_201611091426_0052_m_000000
2016-11-09 14:26:40,342  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0052_m_000000_52: Committed
2016-11-09 14:26:40,346  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 52.0 (TID 52). 1822 bytes result sent to driver
2016-11-09 14:26:40,348  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 52.0 (TID 52) in 119 ms on localhost (1/1)
2016-11-09 14:26:40,348  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 52 (saveAsTextFile at KMeansModel.scala:131) finished in 0.119 s
2016-11-09 14:26:40,348  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 52.0, whose tasks have all completed, from pool 
2016-11-09 14:26:40,348  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 33 finished: saveAsTextFile at KMeansModel.scala:131, took 0.160266 s
2016-11-09 14:26:40,614  INFO  org.apache.spark.storage.BlockManager - logInfo: Removing RDD 7
2016-11-09 14:26:40,614  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned RDD 7
2016-11-09 14:26:40,615  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_78_piece0 on localhost:30044 in memory (size: 26.4 KB, free: 529.9 MB)
2016-11-09 14:26:40,615  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 146
2016-11-09 14:26:40,616  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_77_piece0 on localhost:30044 in memory (size: 6.8 KB, free: 529.9 MB)
2016-11-09 14:26:40,616  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 145
2016-11-09 14:26:40,616  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_76_piece0 on localhost:30044 in memory (size: 246.0 B, free: 529.9 MB)
2016-11-09 14:26:40,617  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_75_piece0 on localhost:30044 in memory (size: 7.1 KB, free: 529.9 MB)
2016-11-09 14:26:40,617  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 144
2016-11-09 14:26:40,618  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_74_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:40,618  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 143
2016-11-09 14:26:40,618  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_73_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,618  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 142
2016-11-09 14:26:40,619  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 18
2016-11-09 14:26:40,619  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_72_piece0 on localhost:30044 in memory (size: 226.0 B, free: 529.9 MB)
2016-11-09 14:26:40,619  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 141
2016-11-09 14:26:40,619  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 140
2016-11-09 14:26:40,620  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_71_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:40,620  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 139
2016-11-09 14:26:40,620  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_70_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,621  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 138
2016-11-09 14:26:40,621  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 17
2016-11-09 14:26:40,621  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_69_piece0 on localhost:30044 in memory (size: 226.0 B, free: 529.9 MB)
2016-11-09 14:26:40,621  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 137
2016-11-09 14:26:40,621  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 136
2016-11-09 14:26:40,622  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_68_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:40,622  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 135
2016-11-09 14:26:40,622  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_67_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,622  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 134
2016-11-09 14:26:40,623  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 16
2016-11-09 14:26:40,623  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_66_piece0 on localhost:30044 in memory (size: 225.0 B, free: 529.9 MB)
2016-11-09 14:26:40,623  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 133
2016-11-09 14:26:40,623  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 132
2016-11-09 14:26:40,624  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_65_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 529.9 MB)
2016-11-09 14:26:40,624  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 131
2016-11-09 14:26:40,624  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_64_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,624  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 130
2016-11-09 14:26:40,625  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 15
2016-11-09 14:26:40,625  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_63_piece0 on localhost:30044 in memory (size: 239.0 B, free: 529.9 MB)
2016-11-09 14:26:40,625  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 129
2016-11-09 14:26:40,625  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 128
2016-11-09 14:26:40,625  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_62_piece0 on localhost:30044 in memory (size: 1659.0 B, free: 529.9 MB)
2016-11-09 14:26:40,626  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 127
2016-11-09 14:26:40,626  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_61_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,626  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 126
2016-11-09 14:26:40,627  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 14
2016-11-09 14:26:40,627  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_60_piece0 on localhost:30044 in memory (size: 291.0 B, free: 529.9 MB)
2016-11-09 14:26:40,627  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 125
2016-11-09 14:26:40,627  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 124
2016-11-09 14:26:40,627  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 123
2016-11-09 14:26:40,627  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_59_piece0 on localhost:30044 in memory (size: 1659.0 B, free: 529.9 MB)
2016-11-09 14:26:40,628  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 122
2016-11-09 14:26:40,628  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_58_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 530.0 MB)
2016-11-09 14:26:40,628  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 121
2016-11-09 14:26:40,628  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 13
2016-11-09 14:26:40,629  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_57_piece0 on localhost:30044 in memory (size: 327.0 B, free: 530.0 MB)
2016-11-09 14:26:40,629  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 120
2016-11-09 14:26:40,629  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 119
2016-11-09 14:26:40,629  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 118
2016-11-09 14:26:40,629  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 117
2016-11-09 14:26:40,629  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_56_piece0 on localhost:30044 in memory (size: 1660.0 B, free: 530.0 MB)
2016-11-09 14:26:40,630  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 116
2016-11-09 14:26:40,630  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_55_piece0 on localhost:30044 in memory (size: 7.7 KB, free: 530.0 MB)
2016-11-09 14:26:40,630  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 115
2016-11-09 14:26:40,630  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned shuffle 12
2016-11-09 14:26:40,630  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Removed broadcast_54_piece0 on localhost:30044 in memory (size: 332.0 B, free: 530.0 MB)
2016-11-09 14:26:40,631  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 114
2016-11-09 14:26:40,631  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 113
2016-11-09 14:26:40,631  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 112
2016-11-09 14:26:40,631  INFO  org.apache.spark.ContextCleaner - logInfo: Cleaned accumulator 111
2016-11-09 14:26:40,733  INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetRelation - logInfo: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,850  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,851  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,851  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,920  INFO  org.apache.spark.SparkContext - logInfo: Starting job: parquet at KMeansModel.scala:135
2016-11-09 14:26:40,921  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Got job 34 (parquet at KMeansModel.scala:135) with 5 output partitions
2016-11-09 14:26:40,921  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Final stage: ResultStage 53 (parquet at KMeansModel.scala:135)
2016-11-09 14:26:40,921  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Parents of final stage: List()
2016-11-09 14:26:40,921  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Missing parents: List()
2016-11-09 14:26:40,922  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting ResultStage 53 (MapPartitionsRDD[76] at rddToDataFrameHolder at KMeansModel.scala:132), which has no missing parents
2016-11-09 14:26:40,957  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_79 stored as values in memory (estimated size 76.9 KB, free 754.6 KB)
2016-11-09 14:26:40,960  INFO  org.apache.spark.storage.MemoryStore - logInfo: Block broadcast_79_piece0 stored as bytes in memory (estimated size 27.7 KB, free 782.2 KB)
2016-11-09 14:26:40,960  INFO  org.apache.spark.storage.BlockManagerInfo - logInfo: Added broadcast_79_piece0 in memory on localhost:30044 (size: 27.7 KB, free: 529.9 MB)
2016-11-09 14:26:40,961  INFO  org.apache.spark.SparkContext - logInfo: Created broadcast 79 from broadcast at DAGScheduler.scala:1006
2016-11-09 14:26:40,961  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Submitting 5 missing tasks from ResultStage 53 (MapPartitionsRDD[76] at rddToDataFrameHolder at KMeansModel.scala:132)
2016-11-09 14:26:40,961  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Adding task set 53.0 with 5 tasks
2016-11-09 14:26:40,963  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 0.0 in stage 53.0 (TID 53, localhost, partition 0,PROCESS_LOCAL, 2622 bytes)
2016-11-09 14:26:40,964  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 1.0 in stage 53.0 (TID 54, localhost, partition 1,PROCESS_LOCAL, 2622 bytes)
2016-11-09 14:26:40,966  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 2.0 in stage 53.0 (TID 55, localhost, partition 2,PROCESS_LOCAL, 2622 bytes)
2016-11-09 14:26:40,967  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 3.0 in stage 53.0 (TID 56, localhost, partition 3,PROCESS_LOCAL, 2622 bytes)
2016-11-09 14:26:40,969  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Starting task 4.0 in stage 53.0 (TID 57, localhost, partition 4,PROCESS_LOCAL, 2622 bytes)
2016-11-09 14:26:40,969  INFO  org.apache.spark.executor.Executor - logInfo: Running task 0.0 in stage 53.0 (TID 53)
2016-11-09 14:26:40,969  INFO  org.apache.spark.executor.Executor - logInfo: Running task 1.0 in stage 53.0 (TID 54)
2016-11-09 14:26:40,970  INFO  org.apache.spark.executor.Executor - logInfo: Running task 2.0 in stage 53.0 (TID 55)
2016-11-09 14:26:40,970  INFO  org.apache.spark.executor.Executor - logInfo: Running task 3.0 in stage 53.0 (TID 56)
2016-11-09 14:26:40,970  INFO  org.apache.spark.executor.Executor - logInfo: Running task 4.0 in stage 53.0 (TID 57)
2016-11-09 14:26:40,993  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,994  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,994  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,996  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,996  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,996  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,996  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,997  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,997  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,997  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,997  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,997  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,998  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,997  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
2016-11-09 14:26:40,998  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - <init>: File Output Committer Algorithm version is 1
2016-11-09 14:26:40,999  INFO  parquet.hadoop.codec.CodecConfig - info: Compression: GZIP
2016-11-09 14:26:40,999  INFO  parquet.hadoop.codec.CodecConfig - info: Compression: GZIP
2016-11-09 14:26:40,999  INFO  parquet.hadoop.codec.CodecConfig - info: Compression: GZIP
2016-11-09 14:26:40,999  INFO  parquet.hadoop.codec.CodecConfig - info: Compression: GZIP
2016-11-09 14:26:41,000  INFO  parquet.hadoop.codec.CodecConfig - info: Compression: GZIP
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet block size to 134217728
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet block size to 134217728
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet page size to 1048576
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet block size to 134217728
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet block size to 134217728
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet block size to 134217728
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet page size to 1048576
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet dictionary page size to 1048576
2016-11-09 14:26:41,003  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Dictionary is on
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet dictionary page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet dictionary page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Dictionary is on
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Dictionary is on
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Validation is off
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet dictionary page size to 1048576
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Validation is off
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Validation is off
2016-11-09 14:26:41,004  INFO  parquet.hadoop.ParquetOutputFormat - info: Parquet dictionary page size to 1048576
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Writer version is: PARQUET_1_0
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Writer version is: PARQUET_1_0
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Writer version is: PARQUET_1_0
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Dictionary is on
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Maximum row group padding size is 8388608 bytes
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Maximum row group padding size is 8388608 bytes
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Maximum row group padding size is 8388608 bytes
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Dictionary is on
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Validation is off
2016-11-09 14:26:41,005  INFO  parquet.hadoop.ParquetOutputFormat - info: Validation is off
2016-11-09 14:26:41,006  INFO  parquet.hadoop.ParquetOutputFormat - info: Writer version is: PARQUET_1_0
2016-11-09 14:26:41,006  INFO  parquet.hadoop.ParquetOutputFormat - info: Writer version is: PARQUET_1_0
2016-11-09 14:26:41,006  INFO  parquet.hadoop.ParquetOutputFormat - info: Maximum row group padding size is 8388608 bytes
2016-11-09 14:26:41,006  INFO  parquet.hadoop.ParquetOutputFormat - info: Maximum row group padding size is 8388608 bytes
2016-11-09 14:26:41,036  INFO  org.apache.spark.sql.execution.datasources.parquet.CatalystWriteSupport - logInfo: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated int32 array;
    }
    optional group values (LIST) {
      repeated double array;
    }
  }
}

       
2016-11-09 14:26:41,036  INFO  org.apache.spark.sql.execution.datasources.parquet.CatalystWriteSupport - logInfo: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated int32 array;
    }
    optional group values (LIST) {
      repeated double array;
    }
  }
}

       
2016-11-09 14:26:41,037  INFO  org.apache.spark.sql.execution.datasources.parquet.CatalystWriteSupport - logInfo: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated int32 array;
    }
    optional group values (LIST) {
      repeated double array;
    }
  }
}

       
2016-11-09 14:26:41,037  INFO  org.apache.spark.sql.execution.datasources.parquet.CatalystWriteSupport - logInfo: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated int32 array;
    }
    optional group values (LIST) {
      repeated double array;
    }
  }
}

       
2016-11-09 14:26:41,037  INFO  org.apache.spark.sql.execution.datasources.parquet.CatalystWriteSupport - logInfo: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated int32 array;
    }
    optional group values (LIST) {
      repeated double array;
    }
  }
}

       
2016-11-09 14:26:41,075  INFO  org.apache.hadoop.io.compress.zlib.ZlibFactory - <clinit>: Successfully loaded & initialized native-zlib library
2016-11-09 14:26:41,077  INFO  org.apache.hadoop.io.compress.CodecPool - getCompressor: Got brand-new compressor [.gz]
2016-11-09 14:26:41,087  INFO  org.apache.hadoop.io.compress.CodecPool - getCompressor: Got brand-new compressor [.gz]
2016-11-09 14:26:41,088  INFO  org.apache.hadoop.io.compress.CodecPool - getCompressor: Got brand-new compressor [.gz]
2016-11-09 14:26:41,088  INFO  org.apache.hadoop.io.compress.CodecPool - getCompressor: Got brand-new compressor [.gz]
2016-11-09 14:26:41,087  INFO  org.apache.hadoop.io.compress.CodecPool - getCompressor: Got brand-new compressor [.gz]
2016-11-09 14:26:41,191  INFO  parquet.hadoop.InternalParquetRecordWriter - info: Flushing mem columnStore to file. allocated memory: 28
2016-11-09 14:26:41,191  INFO  parquet.hadoop.InternalParquetRecordWriter - info: Flushing mem columnStore to file. allocated memory: 28
2016-11-09 14:26:41,191  INFO  parquet.hadoop.InternalParquetRecordWriter - info: Flushing mem columnStore to file. allocated memory: 28
2016-11-09 14:26:41,191  INFO  parquet.hadoop.InternalParquetRecordWriter - info: Flushing mem columnStore to file. allocated memory: 28
2016-11-09 14:26:41,191  INFO  parquet.hadoop.InternalParquetRecordWriter - info: Flushing mem columnStore to file. allocated memory: 28
2016-11-09 14:26:41,231  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [id] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,231  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [id] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,232  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [id] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,231  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [id] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,232  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [id] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [point, type] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [point, type] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [point, type] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 48B for [point, size] INT32: 1 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 48B for [point, size] INT32: 1 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [point, type] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 51B for [point, indices, array] INT32: 1 values, 13B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 61B for [point, type] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 79B for [point, values, array] DOUBLE: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 48B for [point, size] INT32: 1 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 51B for [point, indices, array] INT32: 1 values, 13B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,233  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 48B for [point, size] INT32: 1 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 51B for [point, indices, array] INT32: 1 values, 13B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 77B for [point, values, array] DOUBLE: 1 values, 21B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 79B for [point, values, array] DOUBLE: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 48B for [point, size] INT32: 1 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
2016-11-09 14:26:41,234  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 51B for [point, indices, array] INT32: 1 values, 13B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,235  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 51B for [point, indices, array] INT32: 1 values, 13B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,235  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 79B for [point, values, array] DOUBLE: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,235  INFO  parquet.hadoop.ColumnChunkPageWriteStore - info: written 76B for [point, values, array] DOUBLE: 1 values, 21B raw, 35B comp, 1 pages, encodings: [RLE, PLAIN]
2016-11-09 14:26:41,425  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0053_m_000002_0' to hdfs://nameservice1/usr/machine_learning/model/k_means/data/_temporary/0/task_201611091426_0053_m_000002
2016-11-09 14:26:41,425  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0053_m_000002_0: Committed
2016-11-09 14:26:41,427  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 2.0 in stage 53.0 (TID 55). 873 bytes result sent to driver
2016-11-09 14:26:41,429  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 2.0 in stage 53.0 (TID 55) in 465 ms on localhost (1/5)
2016-11-09 14:26:41,430  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0053_m_000004_0' to hdfs://nameservice1/usr/machine_learning/model/k_means/data/_temporary/0/task_201611091426_0053_m_000004
2016-11-09 14:26:41,430  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0053_m_000004_0: Committed
2016-11-09 14:26:41,432  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 4.0 in stage 53.0 (TID 57). 873 bytes result sent to driver
2016-11-09 14:26:41,433  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 4.0 in stage 53.0 (TID 57) in 466 ms on localhost (2/5)
2016-11-09 14:26:41,442  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0053_m_000001_0' to hdfs://nameservice1/usr/machine_learning/model/k_means/data/_temporary/0/task_201611091426_0053_m_000001
2016-11-09 14:26:41,442  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0053_m_000001_0: Committed
2016-11-09 14:26:41,442  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0053_m_000003_0' to hdfs://nameservice1/usr/machine_learning/model/k_means/data/_temporary/0/task_201611091426_0053_m_000003
2016-11-09 14:26:41,442  INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - commitTask: Saved output of task 'attempt_201611091426_0053_m_000000_0' to hdfs://nameservice1/usr/machine_learning/model/k_means/data/_temporary/0/task_201611091426_0053_m_000000
2016-11-09 14:26:41,442  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0053_m_000003_0: Committed
2016-11-09 14:26:41,443  INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - logInfo: attempt_201611091426_0053_m_000000_0: Committed
2016-11-09 14:26:41,443  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 1.0 in stage 53.0 (TID 54). 873 bytes result sent to driver
2016-11-09 14:26:41,444  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 3.0 in stage 53.0 (TID 56). 873 bytes result sent to driver
2016-11-09 14:26:41,444  INFO  org.apache.spark.executor.Executor - logInfo: Finished task 0.0 in stage 53.0 (TID 53). 873 bytes result sent to driver
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 1.0 in stage 53.0 (TID 54) in 482 ms on localhost (3/5)
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 3.0 in stage 53.0 (TID 56) in 479 ms on localhost (4/5)
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.TaskSetManager - logInfo: Finished task 0.0 in stage 53.0 (TID 53) in 484 ms on localhost (5/5)
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: ResultStage 53 (parquet at KMeansModel.scala:135) finished in 0.484 s
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.TaskSchedulerImpl - logInfo: Removed TaskSet 53.0, whose tasks have all completed, from pool 
2016-11-09 14:26:41,445  INFO  org.apache.spark.scheduler.DAGScheduler - logInfo: Job 34 finished: parquet at KMeansModel.scala:135, took 0.524953 s
2016-11-09 14:26:41,572  INFO  parquet.hadoop.ParquetFileReader - info: Initiating action with parallelism: 5
2016-11-09 14:26:41,681  INFO  org.apache.spark.sql.execution.datasources.DefaultWriterContainer - logInfo: Job job_201611091426_0000 committed.
2016-11-09 14:26:41,684  INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetRelation - logInfo: Listing hdfs://nameservice1/usr/machine_learning/model/k_means/data on driver
2016-11-09 14:26:41,692  INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetRelation - logInfo: Listing hdfs://nameservice1/usr/machine_learning/model/k_means/data on driver
2016-11-09 14:26:41,712  INFO  org.apache.spark.SparkContext - logInfo: Invoking stop() from shutdown hook
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/SQL,null}
2016-11-09 14:26:41,796  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/api,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/static,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-11-09 14:26:41,797  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-11-09 14:26:41,798  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-11-09 14:26:41,799  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-11-09 14:26:41,800  INFO  org.spark-project.jetty.server.handler.ContextHandler - doStop: stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-11-09 14:26:41,851  WARN  org.spark-project.jetty.util.thread.QueuedThreadPool - doStop: 1 threads could not be stopped
2016-11-09 14:26:41,851  INFO  org.spark-project.jetty.util.thread.QueuedThreadPool - doStop: Couldn't stop Thread[qtp1911168986-138,5,main]
2016-11-09 14:26:41,852  INFO  org.apache.spark.ui.SparkUI - logInfo: Stopped Spark web UI at http://192.168.100.103:4040
2016-11-09 14:26:41,899  INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - logInfo: MapOutputTrackerMasterEndpoint stopped!
2016-11-09 14:26:41,911  INFO  org.apache.spark.storage.MemoryStore - logInfo: MemoryStore cleared
2016-11-09 14:26:41,911  INFO  org.apache.spark.storage.BlockManager - logInfo: BlockManager stopped
2016-11-09 14:26:41,911  INFO  org.apache.spark.storage.BlockManagerMaster - logInfo: BlockManagerMaster stopped
2016-11-09 14:26:41,913  INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - logInfo: OutputCommitCoordinator stopped!
2016-11-09 14:26:41,917  WARN  org.apache.spark.rpc.netty.Dispatcher - logWarning: Message RemoteProcessDisconnected(hadoop103.dategeek.com.cn:65171) dropped. RpcEnv already stopped.
2016-11-09 14:26:41,917  INFO  org.apache.spark.SparkContext - logInfo: Successfully stopped SparkContext
2016-11-09 14:26:41,917  INFO  org.apache.spark.util.ShutdownHookManager - logInfo: Shutdown hook called
2016-11-09 14:26:41,918  INFO  org.apache.spark.util.ShutdownHookManager - logInfo: Deleting directory /tmp/spark-bc196639-ea6f-4edf-b717-71fcc5e9aa93
2016-11-09 14:26:41,918  INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - apply$mcV$sp: Shutting down remote daemon.
2016-11-09 14:26:41,919  INFO  org.apache.spark.util.ShutdownHookManager - logInfo: Deleting directory /tmp/spark-0a850c84-c9e3-47bc-86c1-a2e244ece4dc
2016-11-09 14:26:41,921  INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - apply$mcV$sp: Remote daemon shut down; proceeding with flushing remote transports.
2016-11-09 14:26:41,941  INFO  Remoting - apply$mcV$sp: Remoting shut down
2016-11-09 14:26:41,941  INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - apply$mcV$sp: Remoting shut down.
